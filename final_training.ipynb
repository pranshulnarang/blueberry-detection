{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjZ8XeA4NkNe",
    "outputId": "fc453a03-5917-440d-a46f-49604e26a5ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras==2.2.4 in /home/pranshul/.local/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: pyyaml in /home/pranshul/.local/lib/python3.6/site-packages (from keras==2.2.4) (5.3.1)\n",
      "Requirement already satisfied: h5py in /home/pranshul/.local/lib/python3.6/site-packages (from keras==2.2.4) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/pranshul/.local/lib/python3.6/site-packages (from keras==2.2.4) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/pranshul/.local/lib/python3.6/site-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/pranshul/.local/lib/python3.6/site-packages (from keras==2.2.4) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/pranshul/.local/lib/python3.6/site-packages (from keras==2.2.4) (1.19.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/pranshul/.local/lib/python3.6/site-packages (from keras==2.2.4) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# Our YOLOv3 implementation calls for this Keras version\n",
    "!pip3 install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xD-3iDkNlTN",
    "outputId": "c0e02102-2728-4728-8ede-6a572411f1bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: h5py<3.0.0 in /home/pranshul/.local/lib/python3.6/site-packages (2.10.0)\n",
      "Requirement already satisfied: six in /home/pranshul/.local/lib/python3.6/site-packages (from h5py<3.0.0) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /home/pranshul/.local/lib/python3.6/site-packages (from h5py<3.0.0) (1.19.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'h5py<3.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-jscf_XNqsI",
    "outputId": "88cc4957-c383-4e6f-a622-bfac88db1c7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "# use TF 1.x\n",
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKPn_S2uNyjP",
    "outputId": "b65014f4-2ece-4938-8c7f-a74dd76adcd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"<string>\", line 1, in <module>\r\n",
      "ImportError: No module named keras\r\n"
     ]
    }
   ],
   "source": [
    "# Verify our version is correct\n",
    "!python -c 'import keras; print(keras.__version__)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aH5cMtGrN01A",
    "outputId": "c4b0624b-dc79-4859-eb32-f2a0c230e3b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'keras-yolo3'...\n",
      "remote: Enumerating objects: 169, done.\u001b[K\n",
      "remote: Total 169 (delta 0), reused 0 (delta 0), pack-reused 169\u001b[K\n",
      "Receiving objects: 100% (169/169), 172.74 KiB | 1.99 MiB/s, done.\n",
      "Resolving deltas: 100% (80/80), done.\n"
     ]
    }
   ],
   "source": [
    "# Next, we'll grab all the code from our repository of interest \n",
    "!git clone https://github.com/roboflow-ai/keras-yolo3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwS6U5yeN5zK",
    "outputId": "69f91a9f-5da2-4d65-dcf2-4e912732f2a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pranshul/ft4/train/final_train/keras-yolo3\n"
     ]
    }
   ],
   "source": [
    "# change directory to the repo we cloned\n",
    "%cd keras-yolo3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jc8TuaC6N8cY",
    "outputId": "70f08ba4-f343-401d-e970-9f4988946122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   892  100   892    0     0    888      0  0:00:01  0:00:01 --:--:--   888\n",
      "100  180M  100  180M    0     0  8320k      0  0:00:22  0:00:22 --:--:-- 9740k\n",
      "Archive:  roboflow.zip\n",
      " extracting: README.roboflow.txt     \n",
      "   creating: test/\n",
      " extracting: test/_annotations.txt   \n",
      " extracting: test/_classes.txt       \n",
      " extracting: test/frame108_jpg.rf.633ff4e2c7ea13ff1bac68b1de2381f1.jpg  \n",
      " extracting: test/frame11_jpg.rf.8aaee29df344c55dae0370573fa51644.jpg  \n",
      " extracting: test/frame158_jpg.rf.5ab050ef1c16e3c44b81634c07ce9bc9.jpg  \n",
      " extracting: test/frame178_jpg.rf.80ec9d2f2c1c1fba35ac18e6f61c03bf.jpg  \n",
      " extracting: test/frame182_jpg.rf.be2e55bea7ce2fcd0148e13f67ffe17e.jpg  \n",
      " extracting: test/frame191_jpg.rf.a41c8bae96051a81a90e99e5c27c83c1.jpg  \n",
      " extracting: test/frame2_jpg.rf.ae33cbef469da8cc9cec1291ff0491ce.jpg  \n",
      " extracting: test/frame47_jpg.rf.72da30db95ec8e8b1056f383c42e3855.jpg  \n",
      " extracting: test/frame55_jpg.rf.b86cfb17f5795c56fea930ef0bc50613.jpg  \n",
      " extracting: test/frame70_jpg.rf.b22ac77899e160fffd97f47a723f85ac.jpg  \n",
      "   creating: train/\n",
      " extracting: train/_annotations.txt  \n",
      " extracting: train/_classes.txt      \n",
      " extracting: train/frame0_jpg.rf.e345ae36b784ff18150d2f706f0060e1.jpg  \n",
      " extracting: train/frame0_jpg.rf.ecbdd742eab1a107ee6f1b53ba06c032.jpg  \n",
      " extracting: train/frame0_jpg.rf.f4e70e9a6098f1549b62b1eb1438e3b3.jpg  \n",
      " extracting: train/frame101_jpg.rf.78bf5eb2e20603d2690aaba29164a989.jpg  \n",
      " extracting: train/frame101_jpg.rf.7bcbe68432786af15cc44933af64a734.jpg  \n",
      " extracting: train/frame101_jpg.rf.fb3d65be3ebe07d0a680cb19be00fe4d.jpg  \n",
      " extracting: train/frame104_jpg.rf.0e8799a5d25a7df7659e3b4f573e5d7e.jpg  \n",
      " extracting: train/frame104_jpg.rf.4e7e2239d177e8913dee628ae16ccd8a.jpg  \n",
      " extracting: train/frame104_jpg.rf.7ece26c9f82589d8dedf83ad7a297a03.jpg  \n",
      " extracting: train/frame105_jpg.rf.b5ce8cb505824898d95eb18d521ba586.jpg  \n",
      " extracting: train/frame105_jpg.rf.bd2c3e3a59994e48435d6460a35c32b5.jpg  \n",
      " extracting: train/frame105_jpg.rf.c68c277a5397ec222b4e3ab80d5e5886.jpg  \n",
      " extracting: train/frame107_jpg.rf.60e677cb2366a26fe19b0df2cf9ddbfc.jpg  \n",
      " extracting: train/frame107_jpg.rf.69854288e9b4cd7d0ea2567dff59ab42.jpg  \n",
      " extracting: train/frame107_jpg.rf.bec219167208c739badc6d4b66bf1fe7.jpg  \n",
      " extracting: train/frame109_jpg.rf.2ae839f2de2236a8bc00dcc9bb2df0f8.jpg  \n",
      " extracting: train/frame109_jpg.rf.4816a922533c30b94564ac56403b9571.jpg  \n",
      " extracting: train/frame109_jpg.rf.5826e9ee53c06af9952c1437573067f1.jpg  \n",
      " extracting: train/frame10_jpg.rf.14956da9f48d6aa4753e098c5d9941ab.jpg  \n",
      " extracting: train/frame10_jpg.rf.29000935ee4381bfbb453edc56f25d5c.jpg  \n",
      " extracting: train/frame10_jpg.rf.ba8cfaa18d132f1388dd054f46a21bae.jpg  \n",
      " extracting: train/frame110_jpg.rf.04b05409906479792f3cbc75dca76336.jpg  \n",
      " extracting: train/frame110_jpg.rf.a53b403e48028e338a363f28aa9ff026.jpg  \n",
      " extracting: train/frame110_jpg.rf.b0ebce43446bbce5265f9064861b489e.jpg  \n",
      " extracting: train/frame111_jpg.rf.3eaf65e7ecf551f44ba80df9448b02b2.jpg  \n",
      " extracting: train/frame111_jpg.rf.4f74a18860db306edaab5161bdf45b92.jpg  \n",
      " extracting: train/frame111_jpg.rf.6b871e78191a1550d32fc00cef039b54.jpg  \n",
      " extracting: train/frame115_jpg.rf.00a45582218360396d29c4ae2cee9dbe.jpg  \n",
      " extracting: train/frame115_jpg.rf.d0a1065a9f694faac58268491d7d27fc.jpg  \n",
      " extracting: train/frame115_jpg.rf.e047663cbddf4034ad8c375db6e47260.jpg  \n",
      " extracting: train/frame117_jpg.rf.485011efbc3f3da2e7a0dbcc6b6afafa.jpg  \n",
      " extracting: train/frame117_jpg.rf.621c9da475eaf8276ac8c7bb91a0c4bd.jpg  \n",
      " extracting: train/frame117_jpg.rf.86cf16d46aee9c675a03122f6d8b9eae.jpg  \n",
      " extracting: train/frame118_jpg.rf.2dd718d31af2870116f310944b0c6d4c.jpg  \n",
      " extracting: train/frame118_jpg.rf.93e263ebb38e0879b05f0b2bff379324.jpg  \n",
      " extracting: train/frame118_jpg.rf.956c660fb5ffb8d3754f96133e69692d.jpg  \n",
      " extracting: train/frame119_jpg.rf.3b38b4ddceab343028b4adbc680ede3b.jpg  \n",
      " extracting: train/frame119_jpg.rf.6ec54c87667ef4aa0342ee12ed43074b.jpg  \n",
      " extracting: train/frame119_jpg.rf.8ebf1d7f00af512558e44a561c80e45b.jpg  \n",
      " extracting: train/frame120_jpg.rf.54f6f4531946303ebabbe24f388cb115.jpg  \n",
      " extracting: train/frame120_jpg.rf.724b1c6e13fae14edd21c114e3d7d399.jpg  \n",
      " extracting: train/frame120_jpg.rf.b3764a8ee4d791d0b0f679c59cc0ec91.jpg  \n",
      " extracting: train/frame121_jpg.rf.2080b223688a32c47e232aa6330b122e.jpg  \n",
      " extracting: train/frame121_jpg.rf.a7e65fc86bc4bc0365986a3261374a40.jpg  \n",
      " extracting: train/frame121_jpg.rf.d86ce4e45cdb5b3785c0b176c867bd60.jpg  \n",
      " extracting: train/frame122_jpg.rf.2a5ca5a96a04ada26f2d4e89cbec3836.jpg  \n",
      " extracting: train/frame122_jpg.rf.4b8b01b033dce4708bb1c964c81e9c9f.jpg  \n",
      " extracting: train/frame122_jpg.rf.884a9cf342bc5df0776113d2aa83e824.jpg  \n",
      " extracting: train/frame123_jpg.rf.877963c8c5212d9e32e18780acd2f08f.jpg  \n",
      " extracting: train/frame123_jpg.rf.894c8e7e25a021506855ba66dc0bf074.jpg  \n",
      " extracting: train/frame123_jpg.rf.befb35474f4b4b8e45f783d157963a66.jpg  \n",
      " extracting: train/frame124_jpg.rf.1871354667f478f5fc86027305d73b0a.jpg  \n",
      " extracting: train/frame124_jpg.rf.6312c6a7cca8cf583d635c92ce98eaec.jpg  \n",
      " extracting: train/frame124_jpg.rf.da8be5a81992fc74bd1107d32fd0db22.jpg  \n",
      " extracting: train/frame125_jpg.rf.04d55189803b7a3d55d0e2ffc529e46e.jpg  \n",
      " extracting: train/frame125_jpg.rf.52d4a679dee4325b361cb93e796816b1.jpg  \n",
      " extracting: train/frame125_jpg.rf.c366d220173171923c77b526b106c670.jpg  \n",
      " extracting: train/frame126_jpg.rf.8bf59c1bc287cc73937633e2d7256720.jpg  \n",
      " extracting: train/frame126_jpg.rf.9019b39e63d89803fad5b09b8117f3c2.jpg  \n",
      " extracting: train/frame126_jpg.rf.ed3e2ac7a04cef994a720d99045c084e.jpg  \n",
      " extracting: train/frame127_jpg.rf.1be325d55246532248ec46e9728e1d88.jpg  \n",
      " extracting: train/frame127_jpg.rf.2239df4e244b6a2bd6e2944c7acd411b.jpg  \n",
      " extracting: train/frame127_jpg.rf.2d5697b3245007d1f23ce18048c91aeb.jpg  \n",
      " extracting: train/frame128_jpg.rf.32d1c9427e068871eb4d8e80404d7ed2.jpg  \n",
      " extracting: train/frame128_jpg.rf.3d09ec88db606ab711e425598c9d80df.jpg  \n",
      " extracting: train/frame128_jpg.rf.c3c96e866021b3f3a88e5ce312edb933.jpg  \n",
      " extracting: train/frame129_jpg.rf.bcad20ff2b6fbd3edce286fd1364995c.jpg  \n",
      " extracting: train/frame129_jpg.rf.c55715e685352b0cf2c570f2b49d5b23.jpg  \n",
      " extracting: train/frame129_jpg.rf.e95ab0f691179e8ae4c0908815c2d1c4.jpg  \n",
      " extracting: train/frame12_jpg.rf.0521157a3a5aa3965951e5ed98df6836.jpg  \n",
      " extracting: train/frame12_jpg.rf.1448dd70408ad78fa864b55668cd5b47.jpg  \n",
      " extracting: train/frame12_jpg.rf.64bd440bd24a015fc4228b74480609f5.jpg  \n",
      " extracting: train/frame130_jpg.rf.6224e12a505978a53c456d428930f402.jpg  \n",
      " extracting: train/frame130_jpg.rf.a6d53c028aeb8278375bb404a54d8711.jpg  \n",
      " extracting: train/frame130_jpg.rf.d395eecc44e95be426f2635ea1c28749.jpg  \n",
      " extracting: train/frame131_jpg.rf.37fd08a5117ced992158354aca466899.jpg  \n",
      " extracting: train/frame131_jpg.rf.acb977cab90928868670586015039dfb.jpg  \n",
      " extracting: train/frame131_jpg.rf.beffa6e13f3bb63cc3ccb04bf294dc93.jpg  \n",
      " extracting: train/frame132_jpg.rf.8d6445bd53efdd0541a589f193c2a685.jpg  \n",
      " extracting: train/frame132_jpg.rf.8faa1e9a44ddb6b1415e3607479eea6e.jpg  \n",
      " extracting: train/frame132_jpg.rf.a4f673d31cc5d6d51db9c5afd9812e4e.jpg  \n",
      " extracting: train/frame133_jpg.rf.a2055805a74e9438113e4651eb95dffb.jpg  \n",
      " extracting: train/frame133_jpg.rf.e8a1210d682bcbc57442c87f12c16720.jpg  \n",
      " extracting: train/frame133_jpg.rf.f55a02ab9e9f4a85afd0db3d56df2280.jpg  \n",
      " extracting: train/frame137_jpg.rf.a58864eb133bb6f2b2b2e580b704550a.jpg  \n",
      " extracting: train/frame137_jpg.rf.d6212acc27d6c7162c37c45fa578fe45.jpg  \n",
      " extracting: train/frame137_jpg.rf.f0e1246dd4f163eb542bff79bd9e5527.jpg  \n",
      " extracting: train/frame139_jpg.rf.013e7f5e7c12323c0ada07d82596fe42.jpg  \n",
      " extracting: train/frame139_jpg.rf.26c8189d8f193d06c1c1a3d1e4d35ec9.jpg  \n",
      " extracting: train/frame139_jpg.rf.a71caef0400ce9497bb6ff6c7da29020.jpg  \n",
      " extracting: train/frame13_jpg.rf.21b4f2265c63d835d667e2b7e6b1cf28.jpg  \n",
      " extracting: train/frame13_jpg.rf.e8a7dcb32d5109849839dee20e04168a.jpg  \n",
      " extracting: train/frame13_jpg.rf.f16612d5d6ffe1646327c33699431bb5.jpg  \n",
      " extracting: train/frame140_jpg.rf.16b31b865fba2d9369bd1de3517255e6.jpg  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " extracting: train/frame140_jpg.rf.394be3ed12b3d83d6b412fedab573022.jpg  \n",
      " extracting: train/frame140_jpg.rf.42e4bdedbb09d55586f6f79f58b00124.jpg  \n",
      " extracting: train/frame142_jpg.rf.387ac2268d889926f292c8d059314bb3.jpg  \n",
      " extracting: train/frame142_jpg.rf.aaa77cd1c9f3e14263c240995ca35231.jpg  \n",
      " extracting: train/frame142_jpg.rf.d42e4d8fcd9725d72d15e877640bea32.jpg  \n",
      " extracting: train/frame143_jpg.rf.6afb6ffdd525ed178e391f07adccb948.jpg  \n",
      " extracting: train/frame143_jpg.rf.ceabdb8311224ff47b85543cae1fd81f.jpg  \n",
      " extracting: train/frame143_jpg.rf.e4dc3992f0f478bda6a22f2703ff5a89.jpg  \n",
      " extracting: train/frame144_jpg.rf.1473c873b1eb23d866a3158c4a908d4f.jpg  \n",
      " extracting: train/frame144_jpg.rf.1e2246b933d63471acb2de4e8a6e2477.jpg  \n",
      " extracting: train/frame144_jpg.rf.4ba669322665dbc64da411aeede1afff.jpg  \n",
      " extracting: train/frame145_jpg.rf.a5be19c55b5077c6277cc2698e53e8f7.jpg  \n",
      " extracting: train/frame145_jpg.rf.ba9de94c17eba12d7c76c76c932f9e8f.jpg  \n",
      " extracting: train/frame145_jpg.rf.f318d3a2715f27c6b9ed1398fc38d8cc.jpg  \n",
      " extracting: train/frame148_jpg.rf.713f4d8be563c482e5b4edb1a76ea5e3.jpg  \n",
      " extracting: train/frame148_jpg.rf.aad795541b01463548f1258b32138ed0.jpg  \n",
      " extracting: train/frame148_jpg.rf.d4e6727772ad9a636d2747b8b48835ab.jpg  \n",
      " extracting: train/frame149_jpg.rf.906b545df8cc340d6182d2798e5885a5.jpg  \n",
      " extracting: train/frame149_jpg.rf.b86176640c52f49386fd485aba87df1f.jpg  \n",
      " extracting: train/frame149_jpg.rf.c8763f7769352c606039069dce974409.jpg  \n",
      " extracting: train/frame14_jpg.rf.705420503e4844d19314c5d79a9c1203.jpg  \n",
      " extracting: train/frame14_jpg.rf.941cd101d99d8896a01d7bee308bd15b.jpg  \n",
      " extracting: train/frame14_jpg.rf.c776b611bc1241406749dc25e1684a23.jpg  \n",
      " extracting: train/frame150_jpg.rf.323d715a622075b3d2bea92ceabf3a6b.jpg  \n",
      " extracting: train/frame150_jpg.rf.3ad52bc8d5df1aacc679d2a47639172b.jpg  \n",
      " extracting: train/frame150_jpg.rf.8bf60bf0f465ae7686d8067fb7977f54.jpg  \n",
      " extracting: train/frame152_jpg.rf.23f9d83a862a21075751c17b44bc41e7.jpg  \n",
      " extracting: train/frame152_jpg.rf.6784f0692dd4ab3528cb9758323f7bdb.jpg  \n",
      " extracting: train/frame152_jpg.rf.f5f900c31e8ef8e773e0dc5de05412f0.jpg  \n",
      " extracting: train/frame153_jpg.rf.2fab51caf84aebd977313c436aaa8985.jpg  \n",
      " extracting: train/frame153_jpg.rf.6b5c98481872cd4d668ed694a7df8e70.jpg  \n",
      " extracting: train/frame153_jpg.rf.bb8229550fec607a0bb0381b78bc00ba.jpg  \n",
      " extracting: train/frame156_jpg.rf.4bc59d9c7ee1e5fc68c11782c1576a66.jpg  \n",
      " extracting: train/frame156_jpg.rf.9c2c99f2e493a62aecb22e3e0d091291.jpg  \n",
      " extracting: train/frame156_jpg.rf.db974658f820c295851ddecb40a20015.jpg  \n",
      " extracting: train/frame157_jpg.rf.6752dcf40e9d5754887b7dd8fc57ff55.jpg  \n",
      " extracting: train/frame157_jpg.rf.8702a6187aa9aab4420f9b273428c464.jpg  \n",
      " extracting: train/frame157_jpg.rf.a9d2459c3a384298172f0581575558e0.jpg  \n",
      " extracting: train/frame159_jpg.rf.31ae062fd0b092d15965326fb3def7c6.jpg  \n",
      " extracting: train/frame159_jpg.rf.a0987d391a607bcd7425421e0c349783.jpg  \n",
      " extracting: train/frame159_jpg.rf.b5a2a8f683d8579275cc546ecaa45661.jpg  \n",
      " extracting: train/frame15_jpg.rf.4200ee080c4ea66d29f325b8f3a53dcd.jpg  \n",
      " extracting: train/frame15_jpg.rf.5529ba2948da73604f6028556ac43424.jpg  \n",
      " extracting: train/frame15_jpg.rf.d5438fe8c72b7397b82179d6483d7fa0.jpg  \n",
      " extracting: train/frame160_jpg.rf.4c1be650d74fded68d683e677276e716.jpg  \n",
      " extracting: train/frame160_jpg.rf.59de7d5f844ae9a0879612fecebbda86.jpg  \n",
      " extracting: train/frame160_jpg.rf.fffa6e0e08b552af581ac62aecd5b894.jpg  \n",
      " extracting: train/frame161_jpg.rf.6620275e50b43e632b0c5a51fabdb185.jpg  \n",
      " extracting: train/frame161_jpg.rf.960d193214a10cdd005c6481a4f6c694.jpg  \n",
      " extracting: train/frame161_jpg.rf.fb7528d2d53b86fb63cd4c1b4a189c3b.jpg  \n",
      " extracting: train/frame162_jpg.rf.470abcab0e24de18ba71f828bb217b3e.jpg  \n",
      " extracting: train/frame162_jpg.rf.4a65cc9917aa38dc8297cb772f6ede30.jpg  \n",
      " extracting: train/frame162_jpg.rf.7d3d08a80489df27d2816c96aceb72d1.jpg  \n",
      " extracting: train/frame163_jpg.rf.019b15e875bf0ebab1ed2c7cd80f21b4.jpg  \n",
      " extracting: train/frame163_jpg.rf.7bbdb69f57fc9beb78bef610641be096.jpg  \n",
      " extracting: train/frame163_jpg.rf.dfb9844d1c46eea423b8ef0069650448.jpg  \n",
      " extracting: train/frame165_jpg.rf.3ed339d434f9a430f306b5327c1e7eab.jpg  \n",
      " extracting: train/frame165_jpg.rf.58004cf563eb193ac4045fde8d874033.jpg  \n",
      " extracting: train/frame165_jpg.rf.a72de686b533bfbd637fe4b21dc91eff.jpg  \n",
      " extracting: train/frame166_jpg.rf.2291333e8fd5f0e35304242456a8a399.jpg  \n",
      " extracting: train/frame166_jpg.rf.a71609bfaba7f40a8f4d9d7e0a1d7439.jpg  \n",
      " extracting: train/frame166_jpg.rf.ba312c1712acdb73a2f7c72cf51d25e2.jpg  \n",
      " extracting: train/frame167_jpg.rf.693b5626e509d0328f5a89129dc47a28.jpg  \n",
      " extracting: train/frame167_jpg.rf.e25dc269a5ea46b4c7acd8b5e2df84c8.jpg  \n",
      " extracting: train/frame167_jpg.rf.e98c3cfd4865f65ce39dea2e66022d19.jpg  \n",
      " extracting: train/frame168_jpg.rf.6a2c4cc1e20918a05d08c7087826cd5e.jpg  \n",
      " extracting: train/frame168_jpg.rf.6d7c8af2fa9d131b0bce46eba960121d.jpg  \n",
      " extracting: train/frame168_jpg.rf.79e5870adb303b24c94df10d288f141f.jpg  \n",
      " extracting: train/frame169_jpg.rf.5469b55be7d6190a05cde767ef1c2426.jpg  \n",
      " extracting: train/frame169_jpg.rf.ba9949092169ee75bbeee7310c66db59.jpg  \n",
      " extracting: train/frame169_jpg.rf.beb02c1e9c06cd03c6a7c0c1c00b0bfa.jpg  \n",
      " extracting: train/frame16_jpg.rf.6c71c369802a03b79e62cc6c09062bbf.jpg  \n",
      " extracting: train/frame16_jpg.rf.a502e1ae7b088c7880af646cfefab5e5.jpg  \n",
      " extracting: train/frame16_jpg.rf.f7a7452b47e81120a88421f12d8bc293.jpg  \n",
      " extracting: train/frame170_jpg.rf.2514d133a1d8c8badc4d5bd2ac6c04f8.jpg  \n",
      " extracting: train/frame170_jpg.rf.b1131773a12ab7007e7cbeab665782d7.jpg  \n",
      " extracting: train/frame170_jpg.rf.dc0d9749ed3d6cf0c65d457827f4dfc0.jpg  \n",
      " extracting: train/frame171_jpg.rf.511c4459aea132190a4d8e40b2960ec0.jpg  \n",
      " extracting: train/frame171_jpg.rf.87906fa0426147aced283da0e2f39b4c.jpg  \n",
      " extracting: train/frame171_jpg.rf.ab6f5a98d893447b010ae007ff3b5954.jpg  \n",
      " extracting: train/frame173_jpg.rf.7c83faabcae9376ee70894903ea1a294.jpg  \n",
      " extracting: train/frame173_jpg.rf.9cab064adaecc40c1895133b0eef46cc.jpg  \n",
      " extracting: train/frame173_jpg.rf.e6160f1bebf2a1451deabe32dd1da7e6.jpg  \n",
      " extracting: train/frame174_jpg.rf.5e2086e7213e97bdcfb98cb1b61be1f7.jpg  \n",
      " extracting: train/frame174_jpg.rf.62c6938cece7a960489685f6f9a4b2cb.jpg  \n",
      " extracting: train/frame174_jpg.rf.9ee19156bc117cb04ba80556a8602069.jpg  \n",
      " extracting: train/frame176_jpg.rf.32e4060e4b400ecdcfd60e88bf6117cc.jpg  \n",
      " extracting: train/frame176_jpg.rf.9a368e5cf5515247abdb62796a54c157.jpg  \n",
      " extracting: train/frame176_jpg.rf.f195753f479c9af3547f5403a4228d50.jpg  \n",
      " extracting: train/frame177_jpg.rf.01673ed3f3f5c9db4057bef705f836d5.jpg  \n",
      " extracting: train/frame177_jpg.rf.af4646a1ac67871eb83baa84ec943a8e.jpg  \n",
      " extracting: train/frame177_jpg.rf.b4c8b9a5ec2172a09d7921df1fd4f86e.jpg  \n",
      " extracting: train/frame179_jpg.rf.8120b6056d8e44224c1c5faf85299a53.jpg  \n",
      " extracting: train/frame179_jpg.rf.a1c863984eac16646c465a7d47bdf1d8.jpg  \n",
      " extracting: train/frame179_jpg.rf.d326e3f65a608937db887aafbd4910e4.jpg  \n",
      " extracting: train/frame17_jpg.rf.3ea774301324a5f0c31f062e7e0bc52b.jpg  \n",
      " extracting: train/frame17_jpg.rf.81bb73422f1700d27deb4b7597395a03.jpg  \n",
      " extracting: train/frame17_jpg.rf.b259cba13813ad6cb7dd554b05861e87.jpg  \n",
      " extracting: train/frame180_jpg.rf.192c764882dfe6cedfda92a966e1a767.jpg  \n",
      " extracting: train/frame180_jpg.rf.4e65f0504dccb99e271f2a79d3efb661.jpg  \n",
      " extracting: train/frame180_jpg.rf.77e08a1044fe4cf50b065956d5742e2e.jpg  \n",
      " extracting: train/frame181_jpg.rf.76ba1d0d2db06255fab62a2ad87b0edc.jpg  \n",
      " extracting: train/frame181_jpg.rf.cc4dab0333d8b3c7366d4aed00f7a6c9.jpg  \n",
      " extracting: train/frame181_jpg.rf.fc265f82c736c6c55a12756d8cc0ecde.jpg  \n",
      " extracting: train/frame183_jpg.rf.0cc15e6dfb4c35d9b0f33f532dead2de.jpg  \n",
      " extracting: train/frame183_jpg.rf.9cd9ee70a2ae09e3475937bbd2d1420a.jpg  \n",
      " extracting: train/frame183_jpg.rf.fa84a1e9235e210dd3748956624003e5.jpg  \n",
      " extracting: train/frame184_jpg.rf.62b83a8727c954ae01344d32bf26361b.jpg  \n",
      " extracting: train/frame184_jpg.rf.d195212e8d499987cceadc01e6a239d4.jpg  \n",
      " extracting: train/frame184_jpg.rf.d2074a27add13661067c98ef347da78a.jpg  \n",
      " extracting: train/frame186_jpg.rf.0359e6973511f0767f9c2a6d67b7d663.jpg  \n",
      " extracting: train/frame186_jpg.rf.5b3cbd17bef8ed0fa8be7a16c9190e01.jpg  \n",
      " extracting: train/frame186_jpg.rf.eae2e1b27f5720b71163c669edd4598e.jpg  \n",
      " extracting: train/frame188_jpg.rf.5080a0965739289573207ae220ee9b76.jpg  \n",
      " extracting: train/frame188_jpg.rf.6d4ed03ffc033af88ed651dd4fb27983.jpg  \n",
      " extracting: train/frame188_jpg.rf.e67f5c5d0994ebd400c21a8f48178c0b.jpg  \n",
      " extracting: train/frame189_jpg.rf.109ff384c8f591830df85be0940341c5.jpg  \n",
      " extracting: train/frame189_jpg.rf.651887655be73a7f880f750feec8b882.jpg  \n",
      " extracting: train/frame189_jpg.rf.88465f74dba09b420cdf78652109a960.jpg  \n",
      " extracting: train/frame190_jpg.rf.506efb069f63e288968953540751c73e.jpg  \n",
      " extracting: train/frame190_jpg.rf.53d1ec9f54c12af8b0749163bf9cdaba.jpg  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " extracting: train/frame190_jpg.rf.94f430f0e0456fc1836254c654baef34.jpg  \n",
      " extracting: train/frame192_jpg.rf.09a16d3babc9cf28cf63ba41e52efa7d.jpg  \n",
      " extracting: train/frame192_jpg.rf.bc760c0f14aa44cf66e471dc5ea92e06.jpg  \n",
      " extracting: train/frame192_jpg.rf.f16caad3e6721612846639f91574eaf4.jpg  \n",
      " extracting: train/frame193_jpg.rf.77f00367b4a4066b099f68bbee763984.jpg  \n",
      " extracting: train/frame193_jpg.rf.abbe20ae492d1e2064550c4567e4eacf.jpg  \n",
      " extracting: train/frame193_jpg.rf.ce36f146f9daba260ba54171030d4873.jpg  \n",
      " extracting: train/frame195_jpg.rf.08a4734d192be433026f5ae038df948e.jpg  \n",
      " extracting: train/frame195_jpg.rf.98638571aae481114463135cbf8534cb.jpg  \n",
      " extracting: train/frame195_jpg.rf.abd3c265658b26a446bc0a5808da55cf.jpg  \n",
      " extracting: train/frame196_jpg.rf.437edbbc6cc81a48efae8f7db4b61b4b.jpg  \n",
      " extracting: train/frame196_jpg.rf.921414d29e2e76f42ffe1dbf7d619edb.jpg  \n",
      " extracting: train/frame196_jpg.rf.c288a05fd258f58cd34af06057ce2ed7.jpg  \n",
      " extracting: train/frame197_jpg.rf.279c368135754c8f9e2ea84f86f26146.jpg  \n",
      " extracting: train/frame197_jpg.rf.b2575cc9f086a4d1b43af62bfa0674f9.jpg  \n",
      " extracting: train/frame197_jpg.rf.e337f0f48a5de983189cd339aa67095e.jpg  \n",
      " extracting: train/frame19_jpg.rf.061f65793fbb32c3990ca78263005f87.jpg  \n",
      " extracting: train/frame19_jpg.rf.c23b5eb00789b68aa1d607e586f78859.jpg  \n",
      " extracting: train/frame19_jpg.rf.c7e570318aaf979be6927870285aa6b7.jpg  \n",
      " extracting: train/frame200_jpg.rf.6c238066d07b0404c9a9b9a6f82f4665.jpg  \n",
      " extracting: train/frame200_jpg.rf.d100d45b6c22486286e5d84687a65d48.jpg  \n",
      " extracting: train/frame200_jpg.rf.ff852bc83970cf46310d4dac2301a31f.jpg  \n",
      " extracting: train/frame20_jpg.rf.7b78ac0284661373c897432af291ce8b.jpg  \n",
      " extracting: train/frame20_jpg.rf.e3b6c7fb4367f0f6586bb3f0b10cb773.jpg  \n",
      " extracting: train/frame20_jpg.rf.f507f1463f298c0330222578b436716f.jpg  \n",
      " extracting: train/frame21_jpg.rf.2c8a39930dc9694986f4f45253fcc298.jpg  \n",
      " extracting: train/frame21_jpg.rf.c37e22c509a0fabf29c139101ed4627a.jpg  \n",
      " extracting: train/frame21_jpg.rf.c98b6bb69052ce5a614603cd709b123f.jpg  \n",
      " extracting: train/frame22_jpg.rf.3f859ae3bab3748eb3f554dc1c83436b.jpg  \n",
      " extracting: train/frame22_jpg.rf.86b21a79a36a3efbc7352b8742f82c97.jpg  \n",
      " extracting: train/frame22_jpg.rf.95b25f9e024e519625ad032e9ed0cf47.jpg  \n",
      " extracting: train/frame23_jpg.rf.5a8fd01690e0612f8064e38beb81d29b.jpg  \n",
      " extracting: train/frame23_jpg.rf.6f9d8d2961f26a7392b13864ee8a4e43.jpg  \n",
      " extracting: train/frame23_jpg.rf.a0e83c7fb1b9d33d25f29b2d0e81ec01.jpg  \n",
      " extracting: train/frame24_jpg.rf.2e0fa55e467c258c36eb185a576d659b.jpg  \n",
      " extracting: train/frame24_jpg.rf.89556ab455912df5ec1312aeba3f1092.jpg  \n",
      " extracting: train/frame24_jpg.rf.a8f00a4b9362e330d4208962793b1666.jpg  \n",
      " extracting: train/frame25_jpg.rf.13d745712f98e7336e55bea498182cb3.jpg  \n",
      " extracting: train/frame25_jpg.rf.3ae396f33d0fce4f0070b318e1249697.jpg  \n",
      " extracting: train/frame25_jpg.rf.f354eeea160b0fdcea836a48a6c1200a.jpg  \n",
      " extracting: train/frame26_jpg.rf.250f1cb52b0ca5dab1681d376797ce2e.jpg  \n",
      " extracting: train/frame26_jpg.rf.66d4631dc43be06e100cd4fcaeb4a460.jpg  \n",
      " extracting: train/frame26_jpg.rf.6c4ce602b15ed49fcd65af20f17f7011.jpg  \n",
      " extracting: train/frame27_jpg.rf.3dedc3e8270201092d02089af7ad319f.jpg  \n",
      " extracting: train/frame27_jpg.rf.588dd219a31d4995149992e948245bde.jpg  \n",
      " extracting: train/frame27_jpg.rf.a095ac47064cffa122fae970940e0223.jpg  \n",
      " extracting: train/frame29_jpg.rf.42e986accfd388fa9854cbdc84c2e77b.jpg  \n",
      " extracting: train/frame29_jpg.rf.ba4bd7edb5fa20b191e2b9fff2bbc150.jpg  \n",
      " extracting: train/frame29_jpg.rf.d9786d48b187114a73306138e414ab2a.jpg  \n",
      " extracting: train/frame30_jpg.rf.b133132121d41c0404f6e6fe8481df6c.jpg  \n",
      " extracting: train/frame30_jpg.rf.c978df3bb993844426abad39d739dd1e.jpg  \n",
      " extracting: train/frame30_jpg.rf.ff2b94ba2d63534de984845e2ecafaaa.jpg  \n",
      " extracting: train/frame31_jpg.rf.1c5178b2d03c4e440619d1dd3c1e9b60.jpg  \n",
      " extracting: train/frame31_jpg.rf.804b759a51c4e0150362d572fca07bf0.jpg  \n",
      " extracting: train/frame31_jpg.rf.c7daecf99d399c6bc90627072e837564.jpg  \n",
      " extracting: train/frame32_jpg.rf.393bb76d7ee5834d457f68891a18d923.jpg  \n",
      " extracting: train/frame32_jpg.rf.bb412cbc6fd31ed35406093a6efd7a1d.jpg  \n",
      " extracting: train/frame32_jpg.rf.d1d43c76d3781755f419fd8b173c8da3.jpg  \n",
      " extracting: train/frame34_jpg.rf.182f513a89c095e64ee62d03c392a351.jpg  \n",
      " extracting: train/frame34_jpg.rf.1ae59799889eba4789bdcb77a785619b.jpg  \n",
      " extracting: train/frame34_jpg.rf.c23884c02dd5ed4853b3a59c35013b8c.jpg  \n",
      " extracting: train/frame36_jpg.rf.3aa1bccff1a86bf24ac6568da6182f8b.jpg  \n",
      " extracting: train/frame36_jpg.rf.54d3dca2b2adcd36a7ee2d3327fdcb7f.jpg  \n",
      " extracting: train/frame36_jpg.rf.d57d443556561b902368ada649181208.jpg  \n",
      " extracting: train/frame37_jpg.rf.2e54a71086aafec4cd57a3d8fbeba051.jpg  \n",
      " extracting: train/frame37_jpg.rf.56c2befe9f5efc2e7a9825014ca4926a.jpg  \n",
      " extracting: train/frame37_jpg.rf.754fa03142a9150901845836c1662b7e.jpg  \n",
      " extracting: train/frame39_jpg.rf.6b20bf4e7305af608cfdb40d0c80341d.jpg  \n",
      " extracting: train/frame39_jpg.rf.a7d6098771c5e7b573bf840d1894de07.jpg  \n",
      " extracting: train/frame39_jpg.rf.d4b0ff10320746cb33804e898494314d.jpg  \n",
      " extracting: train/frame3_jpg.rf.4b0c5d9d9ef0564de3ef1215b603972a.jpg  \n",
      " extracting: train/frame3_jpg.rf.8f2acacca12e5d8a47a96a1a4df454e5.jpg  \n",
      " extracting: train/frame3_jpg.rf.d31ff75b3371fdf3602849def2058801.jpg  \n",
      " extracting: train/frame40_jpg.rf.59f4de9251b3519a2f2abce64224a8a3.jpg  \n",
      " extracting: train/frame40_jpg.rf.70bd8220fd3567457eb65bea98ad8324.jpg  \n",
      " extracting: train/frame40_jpg.rf.f80cc3ea73dde2cd8ee231eb6c6afef6.jpg  \n",
      " extracting: train/frame41_jpg.rf.3e1a7a30ad050dbfb5bbbaf9aa2ff332.jpg  \n",
      " extracting: train/frame41_jpg.rf.e8257ee2dd733f3b80355336a78b6218.jpg  \n",
      " extracting: train/frame41_jpg.rf.f1cb81244db08ac1959d943ea3eff1d4.jpg  \n",
      " extracting: train/frame42_jpg.rf.4f8a4c0be3f74888a760fa096bbba05a.jpg  \n",
      " extracting: train/frame42_jpg.rf.c691cf222c2941acc4758ccae1a374a8.jpg  \n",
      " extracting: train/frame42_jpg.rf.cd366043e7ee10268522659aa2cd63d0.jpg  \n",
      " extracting: train/frame43_jpg.rf.79741195e35472cb1f2b1850dc6840dc.jpg  \n",
      " extracting: train/frame43_jpg.rf.837fac8e243ea36be447f27ca3e316ef.jpg  \n",
      " extracting: train/frame43_jpg.rf.a020e4da0ec116f0fd76fe265acebe29.jpg  \n",
      " extracting: train/frame44_jpg.rf.0835444e3d7600c7290bceb820a30ab5.jpg  \n",
      " extracting: train/frame44_jpg.rf.4da28826882879595e03c4e54534ddd6.jpg  \n",
      " extracting: train/frame44_jpg.rf.dec24828a1840b28454b53d6023ee167.jpg  \n",
      " extracting: train/frame46_jpg.rf.966efaa3bc1a041893bbacc42f0c2289.jpg  \n",
      " extracting: train/frame46_jpg.rf.bbc4079be4be1ec67237aefb91112979.jpg  \n",
      " extracting: train/frame46_jpg.rf.bf9214273fc1f9e3b9dfd871eccbc2a3.jpg  \n",
      " extracting: train/frame48_jpg.rf.810e2e7257a46e149339891a5f9efaa0.jpg  \n",
      " extracting: train/frame48_jpg.rf.98ba1ca4687283d7e318b5cc5e3b8ca4.jpg  \n",
      " extracting: train/frame48_jpg.rf.d5e26d0244ac65c857995eacfed1c92c.jpg  \n",
      " extracting: train/frame49_jpg.rf.50ca25e181ca1d314543e73484ca7661.jpg  \n",
      " extracting: train/frame49_jpg.rf.52858670fa4d7da40885d4b3ec5a20e3.jpg  \n",
      " extracting: train/frame49_jpg.rf.fdd288dd82885c90dff1ddcdd489ca2c.jpg  \n",
      " extracting: train/frame4_jpg.rf.4c5072f2836cb53957114412d1f531aa.jpg  \n",
      " extracting: train/frame4_jpg.rf.56c78ff3c5528f0e7f0e902574ede751.jpg  \n",
      " extracting: train/frame4_jpg.rf.a1ca4dfa8e86fd0cf368badc9dea6652.jpg  \n",
      " extracting: train/frame50_jpg.rf.6e946d0325ce8bf455f9a9b79c361a85.jpg  \n",
      " extracting: train/frame50_jpg.rf.b238a48158400949dd49a55ca7913e7b.jpg  \n",
      " extracting: train/frame50_jpg.rf.df50e59558d792b5494e4c315a229d85.jpg  \n",
      " extracting: train/frame51_jpg.rf.79945b05c490892b67aaef710fbed406.jpg  \n",
      " extracting: train/frame51_jpg.rf.b9f8d887c2439dfc2b6eff6a685533c5.jpg  \n",
      " extracting: train/frame51_jpg.rf.c086fd9a50612e33c92718d439c3fba6.jpg  \n",
      " extracting: train/frame53_jpg.rf.1594f32a04bbd0b672d835a862b83f18.jpg  \n",
      " extracting: train/frame53_jpg.rf.4374d047773f4b75dbf9c6f1111bd3c3.jpg  \n",
      " extracting: train/frame53_jpg.rf.d492d40ba3b3968848270b56023c34b7.jpg  \n",
      " extracting: train/frame56_jpg.rf.4406eb2734a2bc19d72d4e0f62dc0e32.jpg  \n",
      " extracting: train/frame56_jpg.rf.945a3fb1dc0d346ae6a59eb86d5823e8.jpg  \n",
      " extracting: train/frame56_jpg.rf.b37b72d845a038097d2c57e722896adc.jpg  \n",
      " extracting: train/frame57_jpg.rf.89934b28a756cbfb8942da5bc235ba39.jpg  \n",
      " extracting: train/frame57_jpg.rf.ad98ecdda74328e578162294c09fecbe.jpg  \n",
      " extracting: train/frame57_jpg.rf.faedfeea83e21cc01c2fe3d30a675f5b.jpg  \n",
      " extracting: train/frame58_jpg.rf.38a2873497850b9087285dec9d7522b9.jpg  \n",
      " extracting: train/frame58_jpg.rf.7ca23c6fdd6603bc94327c679fd88399.jpg  \n",
      " extracting: train/frame58_jpg.rf.c57992913ada234f419ffb392fca056d.jpg  \n",
      " extracting: train/frame59_jpg.rf.3a1bec80f0a716497927248cfac9971e.jpg  \n",
      " extracting: train/frame59_jpg.rf.7ae357263f48c151e425a9ee774fc50b.jpg  \n",
      " extracting: train/frame59_jpg.rf.819fcc0349a698e4ea930124ecf4bd10.jpg  \n",
      " extracting: train/frame5_jpg.rf.03ea90c34e0bd89ee304046727559bfb.jpg  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " extracting: train/frame5_jpg.rf.3dd2e06758de8655501d97ff0c96cddf.jpg  \n",
      " extracting: train/frame5_jpg.rf.7453f32e55dc134086b99097c1e77ec1.jpg  \n",
      " extracting: train/frame60_jpg.rf.48fa94dc56b8b0458261e76aa0dbba3c.jpg  \n",
      " extracting: train/frame60_jpg.rf.703ab3b6db81e2977de8de941c7bce00.jpg  \n",
      " extracting: train/frame60_jpg.rf.e07800e9ef3dd20debf87fb9c9a4cc86.jpg  \n",
      " extracting: train/frame61_jpg.rf.4030be9481309ce40b45bb7c30e70f98.jpg  \n",
      " extracting: train/frame61_jpg.rf.616a11ec01c7c505488ad4768d9bad9d.jpg  \n",
      " extracting: train/frame61_jpg.rf.a7590dc225f1cb4744bb60306cf52e08.jpg  \n",
      " extracting: train/frame62_jpg.rf.1607617b6bdde523b4b3ac7da49e0752.jpg  \n",
      " extracting: train/frame62_jpg.rf.7fce7a93e9dcc7689e4bb550b5b74db6.jpg  \n",
      " extracting: train/frame62_jpg.rf.bd7269c6946c16e4ed585ef7b47e8572.jpg  \n",
      " extracting: train/frame63_jpg.rf.4475096a97e9be33660a2a12bbcb1363.jpg  \n",
      " extracting: train/frame63_jpg.rf.529b0ee3994e42f503932009dbceafa3.jpg  \n",
      " extracting: train/frame63_jpg.rf.b1978b87c5407185cf0208991ac9737e.jpg  \n",
      " extracting: train/frame64_jpg.rf.2efd9516359d3588dcb80a95f1522d4c.jpg  \n",
      " extracting: train/frame64_jpg.rf.c900326790039476cbdf069f2b1f4481.jpg  \n",
      " extracting: train/frame64_jpg.rf.ecd5ef3edaeb5ef86f8d02912d411498.jpg  \n",
      " extracting: train/frame65_jpg.rf.4b5570ef7e9af135896e942e8f29582e.jpg  \n",
      " extracting: train/frame65_jpg.rf.5f77eaaceb8856bd5122544d2464d86d.jpg  \n",
      " extracting: train/frame65_jpg.rf.e3755830cd20faac53b5c27992f427cf.jpg  \n",
      " extracting: train/frame66_jpg.rf.b2f7a54543cb1f45140b2f16ad1d7f7e.jpg  \n",
      " extracting: train/frame66_jpg.rf.cb50a5e2c07f00b7d60de275e67e180d.jpg  \n",
      " extracting: train/frame66_jpg.rf.fa3e90fc611070b1866c096690b9f804.jpg  \n",
      " extracting: train/frame67_jpg.rf.7e8e8dd2435251f04acae077066a8569.jpg  \n",
      " extracting: train/frame67_jpg.rf.821d796c1a61464c70a1d609aa161427.jpg  \n",
      " extracting: train/frame67_jpg.rf.a171fe860b120934872d4f445668062e.jpg  \n",
      " extracting: train/frame69_jpg.rf.1547d2469b4d326ddc678d121fd40837.jpg  \n",
      " extracting: train/frame69_jpg.rf.2f8a84ce5aea2ceed3a10078133e7a1c.jpg  \n",
      " extracting: train/frame69_jpg.rf.72c47b7a43a96eea75e91eeb52517ce0.jpg  \n",
      " extracting: train/frame71_jpg.rf.6f8a1e8a281ed0583ce5c1b6327c712a.jpg  \n",
      " extracting: train/frame71_jpg.rf.d6d176f0a4c6da6e647ccd6a2ace99db.jpg  \n",
      " extracting: train/frame71_jpg.rf.fa6f7ab18ff8d918553d9143e6d45cdd.jpg  \n",
      " extracting: train/frame72_jpg.rf.1bfd34cb37b07a73ad3604fadfe0943f.jpg  \n",
      " extracting: train/frame72_jpg.rf.54ffb7abc3b28e97d9e0b91485c7fdc9.jpg  \n",
      " extracting: train/frame72_jpg.rf.b5e94bd9fe025c7c931ea4c7fcd525e0.jpg  \n",
      " extracting: train/frame73_jpg.rf.0ce87ec58b901b4cb45d98aa9c90eb10.jpg  \n",
      " extracting: train/frame73_jpg.rf.ada951c33045c9857736bab39e95d16f.jpg  \n",
      " extracting: train/frame73_jpg.rf.fbfd9bb8d41d2dd65f94910a6d2fb823.jpg  \n",
      " extracting: train/frame74_jpg.rf.10e796f55a3fb3654ab2b7ccb5198be1.jpg  \n",
      " extracting: train/frame74_jpg.rf.6de63ad9cb71be1625c422a9f694a728.jpg  \n",
      " extracting: train/frame74_jpg.rf.deeef3cc87dc554e0503c375c1d272af.jpg  \n",
      " extracting: train/frame75_jpg.rf.2bdc858d0ac8e8f4f69588090ee7b494.jpg  \n",
      " extracting: train/frame75_jpg.rf.cb76313d0e2083b17547344c52147e56.jpg  \n",
      " extracting: train/frame75_jpg.rf.e567fb9e25a95728d873eef17b5536f3.jpg  \n",
      " extracting: train/frame76_jpg.rf.018d5c500138e661117a8f2a55812316.jpg  \n",
      " extracting: train/frame76_jpg.rf.6a29ea54403945e5ce3f11804ee1ac1b.jpg  \n",
      " extracting: train/frame76_jpg.rf.e4cc3cfde8715d3ef840fc86cc369a14.jpg  \n",
      " extracting: train/frame77_jpg.rf.51b79298fa746736b3c9d2bbdbb437a5.jpg  \n",
      " extracting: train/frame77_jpg.rf.aedfcbe3ea210c63b5442ce78bcfd437.jpg  \n",
      " extracting: train/frame77_jpg.rf.cdc61f5a58877fba64e118298d10e1ef.jpg  \n",
      " extracting: train/frame78_jpg.rf.41cd7f38b4221aedb42d5db640640906.jpg  \n",
      " extracting: train/frame78_jpg.rf.c3a73f9616794164c74d672a6073dbec.jpg  \n",
      " extracting: train/frame78_jpg.rf.f974692cb611d7ba236038a4ea30a8cd.jpg  \n",
      " extracting: train/frame79_jpg.rf.9c3fab592bee56ce758903a346f90ba0.jpg  \n",
      " extracting: train/frame79_jpg.rf.a240be97ba62217859046b7fc03385bb.jpg  \n",
      " extracting: train/frame79_jpg.rf.b8bd9821f48d3e473b4f56cb81065247.jpg  \n",
      " extracting: train/frame7_jpg.rf.4fbc00667eba5939dfdf3e91a3cca87d.jpg  \n",
      " extracting: train/frame7_jpg.rf.70ba41b2d2bcb779a0aa3535b327547a.jpg  \n",
      " extracting: train/frame7_jpg.rf.92d009e187973390b9454072cbc59421.jpg  \n",
      " extracting: train/frame80_jpg.rf.61ec26d26195132aab653136a9c6d975.jpg  \n",
      " extracting: train/frame80_jpg.rf.d74460c3e72ba513ae92506e03004e82.jpg  \n",
      " extracting: train/frame80_jpg.rf.fcd3a73bd8ed9f92c93865019f097654.jpg  \n",
      " extracting: train/frame82_jpg.rf.9d5cbc89b308861a4a3d5d261f4b3a3c.jpg  \n",
      " extracting: train/frame82_jpg.rf.c33c57679ca3764979d5f7c7dafe91a4.jpg  \n",
      " extracting: train/frame82_jpg.rf.d21cc6d8a09724a084332f3aa0bb9f66.jpg  \n",
      " extracting: train/frame83_jpg.rf.03a54e7d8dd4d06e59dcea4b5445ae57.jpg  \n",
      " extracting: train/frame83_jpg.rf.938bcc0d2faa9fc2716b95d9fff0581f.jpg  \n",
      " extracting: train/frame83_jpg.rf.d6485c3ad90c65af06c4b801895fd5e5.jpg  \n",
      " extracting: train/frame84_jpg.rf.10e731655e21dfccc585d345d63a9b1a.jpg  \n",
      " extracting: train/frame84_jpg.rf.2d272370355e8beec012b9d8bc0e13f2.jpg  \n",
      " extracting: train/frame84_jpg.rf.54493138f70da026d8bd621973a6aec3.jpg  \n",
      " extracting: train/frame87_jpg.rf.4777c90f3d4c034792860e27641da44f.jpg  \n",
      " extracting: train/frame87_jpg.rf.48e6f19e4025402ba84bb182367da128.jpg  \n",
      " extracting: train/frame87_jpg.rf.f9c9f522e3311528170a5d9a9de99d8d.jpg  \n",
      " extracting: train/frame88_jpg.rf.a7f9f7b4514030dc62a1f6494f7f7026.jpg  \n",
      " extracting: train/frame88_jpg.rf.b1dc1f8fa923af62becfba3a97d2853f.jpg  \n",
      " extracting: train/frame88_jpg.rf.d179c598874b0ff20d72c9e8873fcefd.jpg  \n",
      " extracting: train/frame89_jpg.rf.4de30ee3954dc21269981c9344e91863.jpg  \n",
      " extracting: train/frame89_jpg.rf.e77f1a592e5a64e09eb227744c8e0a7c.jpg  \n",
      " extracting: train/frame89_jpg.rf.ee72cffeee34d5cee7bb0928e4e5517f.jpg  \n",
      " extracting: train/frame8_jpg.rf.0432da8406a1404fbc42fb2ee17c60f2.jpg  \n",
      " extracting: train/frame8_jpg.rf.2936b3a9ba01f491f250c99eec3db094.jpg  \n",
      " extracting: train/frame8_jpg.rf.c90cdd6977c85ebbef2b4be522b81b7b.jpg  \n",
      " extracting: train/frame90_jpg.rf.1d9c789e44eff5ed81f1239f78c8549f.jpg  \n",
      " extracting: train/frame90_jpg.rf.a1d03dc6df5e2d37f99c3cb6334ed23f.jpg  \n",
      " extracting: train/frame90_jpg.rf.a38e07a477b525a74a80fa6bdf33c3df.jpg  \n",
      " extracting: train/frame91_jpg.rf.083f8d075af2c7570718e7953639e56c.jpg  \n",
      " extracting: train/frame91_jpg.rf.3e69738e8491cfa9b928e29f50330df7.jpg  \n",
      " extracting: train/frame91_jpg.rf.cc0494133a36b5bf0edfe518a6329eb0.jpg  \n",
      " extracting: train/frame92_jpg.rf.2714e00a2b7a40754d5a49924965654f.jpg  \n",
      " extracting: train/frame92_jpg.rf.4671c8361c3943228a0dce1f1c1301a5.jpg  \n",
      " extracting: train/frame92_jpg.rf.b511ded2271dc95fb4cd3b5840fef4ef.jpg  \n",
      " extracting: train/frame93_jpg.rf.10dbddc4fa8e940f43653b6cffaa108d.jpg  \n",
      " extracting: train/frame93_jpg.rf.afa05c5af550623afb0bd6be907a76e0.jpg  \n",
      " extracting: train/frame93_jpg.rf.d653300acfed5c549a0ad74ab5ddcc21.jpg  \n",
      " extracting: train/frame94_jpg.rf.949d6ec2c946f447e8b95f18fbec95f6.jpg  \n",
      " extracting: train/frame94_jpg.rf.e9a0062dd6c34a73475eba95a2cc27bb.jpg  \n",
      " extracting: train/frame94_jpg.rf.ed0b940dad07f6607fc14109ce5dd148.jpg  \n",
      " extracting: train/frame95_jpg.rf.0b663f5f7ee9b77680088c53197b894c.jpg  \n",
      " extracting: train/frame95_jpg.rf.d2edfc94df3cf7ff7e96014db826bde8.jpg  \n",
      " extracting: train/frame95_jpg.rf.f8cd8526a63191a0ad697e49917b7f19.jpg  \n",
      " extracting: train/frame96_jpg.rf.1969c1d0db093466a811195cc65dbb3f.jpg  \n",
      " extracting: train/frame96_jpg.rf.1ad154d9f986ffb4abf12ca1a618a8c6.jpg  \n",
      " extracting: train/frame96_jpg.rf.b17db6024e4308717f457712133d1873.jpg  \n",
      " extracting: train/frame97_jpg.rf.03bc486e601bb9a5716c78ffcb025029.jpg  \n",
      " extracting: train/frame97_jpg.rf.406e93734c3cab4c9b4ec7c7b7bc80c3.jpg  \n",
      " extracting: train/frame97_jpg.rf.efe2a44de543ee6289e3240eeb5b403d.jpg  \n",
      " extracting: train/frame98_jpg.rf.2740ac162304d1ec2834d7591c394730.jpg  \n",
      " extracting: train/frame98_jpg.rf.617aade0770b6caa701d1ca35bfa8673.jpg  \n",
      " extracting: train/frame98_jpg.rf.e1ee5f4142b95f932aa9424f885a91fb.jpg  \n",
      " extracting: train/frame99_jpg.rf.6fc03bd12d99d33c1a7b81c7049a9786.jpg  \n",
      " extracting: train/frame99_jpg.rf.aa2d55bf9fd5f735e34133f910575dcc.jpg  \n",
      " extracting: train/frame99_jpg.rf.fe87a241c5d59f93fdaa1b188e45d4c9.jpg  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " extracting: train/frame9_jpg.rf.35c794bddee21b7c9af3413cf41d24be.jpg  \n",
      " extracting: train/frame9_jpg.rf.6649e162780dc60e01a1b193c00c1979.jpg  \n",
      " extracting: train/frame9_jpg.rf.8a6f5f45e6678e5382306689001c7eb0.jpg  \n",
      "   creating: valid/\n",
      " extracting: valid/_annotations.txt  \n",
      " extracting: valid/_classes.txt      \n",
      " extracting: valid/frame100_jpg.rf.3ee268b88f9092235b20d574b4debe0a.jpg  \n",
      " extracting: valid/frame102_jpg.rf.511204bbf3b6d7a167ad65d7c6e20d0e.jpg  \n",
      " extracting: valid/frame103_jpg.rf.2901f7fef63e1e0b9b5184c81d53429f.jpg  \n",
      " extracting: valid/frame106_jpg.rf.5ed7e14a07f7dca5342e8cfce77a102d.jpg  \n",
      " extracting: valid/frame112_jpg.rf.f0196d7379bffae249e86caebd3c7c91.jpg  \n",
      " extracting: valid/frame113_jpg.rf.8ceb21901a5c6eeaa490b33487b50464.jpg  \n",
      " extracting: valid/frame114_jpg.rf.85de1f369a85bdf1ce9305d2fab6d082.jpg  \n",
      " extracting: valid/frame116_jpg.rf.1288e76cef13bdcf9fb9898c958e4881.jpg  \n",
      " extracting: valid/frame134_jpg.rf.802b46bc3100835524300ce32e533d9d.jpg  \n",
      " extracting: valid/frame135_jpg.rf.374dce9d09eebba1c89205d3dbfba228.jpg  \n",
      " extracting: valid/frame136_jpg.rf.1a085b53031ab530544ee6d136af646a.jpg  \n",
      " extracting: valid/frame138_jpg.rf.71c040a88e36e3209b9c33e5e14e95e4.jpg  \n",
      " extracting: valid/frame141_jpg.rf.d4a646f6400eac1da8736a14f0eafc4a.jpg  \n",
      " extracting: valid/frame146_jpg.rf.f81c31a677c3c42b4433a38d5ad6dac1.jpg  \n",
      " extracting: valid/frame147_jpg.rf.71182ce96f82555eb0c12c8ffbbe8cf7.jpg  \n",
      " extracting: valid/frame151_jpg.rf.26e87c290945c8c7eea1e13c471bd1dc.jpg  \n",
      " extracting: valid/frame154_jpg.rf.fa07c21a4cae00cbafc9cf79e25fac90.jpg  \n",
      " extracting: valid/frame155_jpg.rf.1119828303405a852a67af4973a77e84.jpg  \n",
      " extracting: valid/frame164_jpg.rf.3302e813f87419e323b87d09e9bfa613.jpg  \n",
      " extracting: valid/frame172_jpg.rf.db7b0e74e691b43a184924558074608a.jpg  \n",
      " extracting: valid/frame175_jpg.rf.6cff1794a3d5b03d5364f51df83f662a.jpg  \n",
      " extracting: valid/frame185_jpg.rf.fefdd123fb7863df1fa09dcecf3660c0.jpg  \n",
      " extracting: valid/frame187_jpg.rf.3753fe1c387282946c55fcbb3d0b46b9.jpg  \n",
      " extracting: valid/frame18_jpg.rf.a646c184cba46d2f42b795390614ba5d.jpg  \n",
      " extracting: valid/frame194_jpg.rf.735beb843a69b8b1e4420f031c830bb2.jpg  \n",
      " extracting: valid/frame198_jpg.rf.57b8eedaf654c4f40867650d3e0d6acf.jpg  \n",
      " extracting: valid/frame199_jpg.rf.6999d9bd1fd3b90a7528326a92a126ec.jpg  \n",
      " extracting: valid/frame1_jpg.rf.a3e95f61050e94a6dd358d2ecd540201.jpg  \n",
      " extracting: valid/frame28_jpg.rf.8d2b03b5889d69fbdcd3ee6a1f4d561a.jpg  \n",
      " extracting: valid/frame33_jpg.rf.c8fbc28557e4dcadd9e7b08e876d6539.jpg  \n",
      " extracting: valid/frame35_jpg.rf.56ea995fa5969c6fd761a2ee7c3f30aa.jpg  \n",
      " extracting: valid/frame38_jpg.rf.a1667a48f802711b856459267ea350b4.jpg  \n",
      " extracting: valid/frame45_jpg.rf.4c9fec0495de0032622a1eb7c60b6b85.jpg  \n",
      " extracting: valid/frame52_jpg.rf.116cdbe5208098e44ac0f8fc0b67afe1.jpg  \n",
      " extracting: valid/frame54_jpg.rf.ad32c3e8dcd747d8715f24a569ea49de.jpg  \n",
      " extracting: valid/frame68_jpg.rf.f7bef89a483244d495ffbbe2d0a0a69e.jpg  \n",
      " extracting: valid/frame6_jpg.rf.6d3fa4909423f4a0a172b6995bb949b3.jpg  \n",
      " extracting: valid/frame81_jpg.rf.d0a90a118f85183ac53945b049638b9d.jpg  \n",
      " extracting: valid/frame85_jpg.rf.2c603217c11f80fa6b8ef5c526204f95.jpg  \n",
      " extracting: valid/frame86_jpg.rf.843c14cdd5bad600a970a76d37cd279b.jpg  \n"
     ]
    }
   ],
   "source": [
    "# Paste Roboflow code from snippet here from above to here! eg !curl -L https://app.roboflow.ai/ds/eOSXbt7KWu?key=YOURKEY | jar -x\n",
    "!curl -L \"https://app.roboflow.com/ds/jWfcLI6yeX?key=Z3mmoUSpvw\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "McDfen83N_po",
    "outputId": "903801a4-558c-4587-a47e-b2351956d81b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pranshul/ft4/train/final_train/keras-yolo3/train\n"
     ]
    }
   ],
   "source": [
    "# change directory into our export folder from Roboflow\n",
    "%cd train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "atPe4ElvODPT"
   },
   "outputs": [],
   "source": [
    "# move everything from the Roboflow export to the root of our keras-yolo3 folder\n",
    "%mv * ../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xymVEoX2OFjI",
    "outputId": "9cdfd5b3-bc6a-42de-ef52-706b52b0d2f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pranshul/ft4/train/final_train/keras-yolo3\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9EMelrebOGqe",
    "outputId": "9f8d630f-ba97-489f-f8c7-64fd7170b077"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-05 01:12:00--  https://pjreddie.com/media/files/yolov3.weights\n",
      "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
      "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248007048 (237M) [application/octet-stream]\n",
      "Saving to: yolov3.weights\n",
      "\n",
      "yolov3.weights      100%[===================>] 236.52M   507KB/s    in 8m 10s  \n",
      "\n",
      "2021-06-05 01:20:12 (494 KB/s) - yolov3.weights saved [248007048/248007048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download our DarkNet weights \n",
    "!wget https://pjreddie.com/media/files/yolov3.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oe0nfgPQOJQo",
    "outputId": "2c7e246b-a535-4477-9507-8d6a59cf234d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'convert.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# call a Python script to set up our architecture with downloaded pre-trained weights\n",
    "!python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C2avQozyOLzO",
    "outputId": "8acb75a6-8b8c-43d8-e8a9-817bc852d474"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------CLASS NAMES-------------------\n",
      "['intermediate', 'mature', 'young']\n",
      "-------------------CLASS NAMES-------------------\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Create YOLOv3 model with 9 anchors and 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tensorflow-1.15.2/python3.7/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((1, 1, 1024, 24) vs (255, 1024, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "/tensorflow-1.15.2/python3.7/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((24,) vs (255,)).\n",
      "  weight_values[i].shape))\n",
      "/tensorflow-1.15.2/python3.7/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((1, 1, 512, 24) vs (255, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "/tensorflow-1.15.2/python3.7/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((24,) vs (255,)).\n",
      "  weight_values[i].shape))\n",
      "/tensorflow-1.15.2/python3.7/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((1, 1, 256, 24) vs (255, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "/tensorflow-1.15.2/python3.7/keras/engine/saving.py:1319: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((24,) vs (255,)).\n",
      "  weight_values[i].shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights model_data/yolo.h5.\n",
      "Freeze the first 249 layers of total 252 layers.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:3170: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 382 samples, val on 95 samples, with batch size 32.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/500\n",
      "11/11 [==============================] - 60s 5s/step - loss: 6104.3354 - val_loss: 2988.5164\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 1962.2689 - val_loss: 1064.6799\n",
      "Epoch 3/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 827.3638 - val_loss: 545.0510\n",
      "Epoch 4/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 479.5789 - val_loss: 335.1991\n",
      "Epoch 5/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 345.7044 - val_loss: 270.3755\n",
      "Epoch 6/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 282.5191 - val_loss: 237.0031\n",
      "Epoch 7/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 244.9418 - val_loss: 204.2354\n",
      "Epoch 8/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 221.0437 - val_loss: 180.9814\n",
      "Epoch 9/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 198.5929 - val_loss: 175.9959\n",
      "Epoch 10/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 178.6246 - val_loss: 158.4859\n",
      "Epoch 11/500\n",
      "11/11 [==============================] - 18s 2s/step - loss: 164.1624 - val_loss: 148.2597\n",
      "Epoch 12/500\n",
      "11/11 [==============================] - 33s 3s/step - loss: 156.6968 - val_loss: 135.0330\n",
      "Epoch 13/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 146.3936 - val_loss: 122.0036\n",
      "Epoch 14/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 140.5279 - val_loss: 114.1819\n",
      "Epoch 15/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 129.1983 - val_loss: 130.1462\n",
      "Epoch 16/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 127.9533 - val_loss: 99.7745\n",
      "Epoch 17/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 125.4553 - val_loss: 115.3927\n",
      "Epoch 18/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 108.6029 - val_loss: 97.4006\n",
      "Epoch 19/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 113.5739 - val_loss: 103.5440\n",
      "Epoch 20/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 115.4313 - val_loss: 105.8865\n",
      "Epoch 21/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 104.4136 - val_loss: 87.2197\n",
      "Epoch 22/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 100.7481 - val_loss: 87.5192\n",
      "Epoch 23/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 104.9029 - val_loss: 96.5977\n",
      "Epoch 24/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 95.3154 - val_loss: 94.8987\n",
      "Epoch 25/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 97.3029 - val_loss: 98.7202\n",
      "Epoch 26/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 94.6548 - val_loss: 101.3863\n",
      "Epoch 27/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 93.8866 - val_loss: 91.0854\n",
      "Epoch 28/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 91.2036 - val_loss: 93.8946\n",
      "Epoch 29/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 90.8874 - val_loss: 93.9145\n",
      "Epoch 30/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 85.4083 - val_loss: 96.8729\n",
      "Epoch 31/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 92.2947 - val_loss: 98.6504\n",
      "Epoch 32/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 81.8216 - val_loss: 97.1388\n",
      "Epoch 33/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 85.3548 - val_loss: 73.9342\n",
      "Epoch 34/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 84.4377 - val_loss: 90.0310\n",
      "Epoch 35/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 83.1474 - val_loss: 81.0270\n",
      "Epoch 36/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 87.5694 - val_loss: 93.2374\n",
      "Epoch 37/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 83.5503 - val_loss: 56.3635\n",
      "Epoch 38/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 80.8431 - val_loss: 87.9520\n",
      "Epoch 39/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 81.5661 - val_loss: 75.1665\n",
      "Epoch 40/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 78.6470 - val_loss: 73.1590\n",
      "Epoch 41/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 82.6126 - val_loss: 80.7680\n",
      "Epoch 42/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 76.3896 - val_loss: 82.4885\n",
      "Epoch 43/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 78.8066 - val_loss: 82.4324\n",
      "Epoch 44/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 76.7555 - val_loss: 91.3541\n",
      "Epoch 45/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 76.5526 - val_loss: 78.7389\n",
      "Epoch 46/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 75.3203 - val_loss: 82.9929\n",
      "Epoch 47/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 77.8551 - val_loss: 70.5537\n",
      "Epoch 48/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 76.3440 - val_loss: 102.8482\n",
      "Epoch 49/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 77.7023 - val_loss: 60.1038\n",
      "Epoch 50/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 76.4611 - val_loss: 71.5106\n",
      "Epoch 51/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 72.7242 - val_loss: 69.2201\n",
      "Epoch 52/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 72.8504 - val_loss: 89.1853\n",
      "Epoch 53/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 73.5511 - val_loss: 72.6342\n",
      "Epoch 54/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 73.5323 - val_loss: 81.8595\n",
      "Epoch 55/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 79.0906 - val_loss: 85.9724\n",
      "Epoch 56/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 69.9961 - val_loss: 78.2305\n",
      "Epoch 57/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 70.6839 - val_loss: 80.5803\n",
      "Epoch 58/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 76.5441 - val_loss: 84.8072\n",
      "Epoch 59/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 77.1469 - val_loss: 63.0500\n",
      "Epoch 60/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 71.3314 - val_loss: 87.8797\n",
      "Epoch 61/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 73.5199 - val_loss: 79.3780\n",
      "Epoch 62/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 70.2350 - val_loss: 79.0947\n",
      "Epoch 63/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 71.6235 - val_loss: 104.4300\n",
      "Epoch 64/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 71.9488 - val_loss: 56.6146\n",
      "Epoch 65/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 72.2427 - val_loss: 110.9640\n",
      "Epoch 66/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 72.4350 - val_loss: 87.9280\n",
      "Epoch 67/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 71.5524 - val_loss: 93.2642\n",
      "Epoch 68/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 75.9905 - val_loss: 80.2600\n",
      "Epoch 69/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.8706 - val_loss: 62.2477\n",
      "Epoch 70/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 71.0509 - val_loss: 68.8782\n",
      "Epoch 71/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 69.5362 - val_loss: 85.5927\n",
      "Epoch 72/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 75.2125 - val_loss: 93.6669\n",
      "Epoch 73/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 69.3883 - val_loss: 75.9613\n",
      "Epoch 74/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 69.4590 - val_loss: 57.0478\n",
      "Epoch 75/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 69.1883 - val_loss: 81.6337\n",
      "Epoch 76/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 72.8872 - val_loss: 63.9815\n",
      "Epoch 77/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 69.4040 - val_loss: 92.9493\n",
      "Epoch 78/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 70.1784 - val_loss: 77.1066\n",
      "Epoch 79/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 69.2580 - val_loss: 78.7584\n",
      "Epoch 80/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 70.6627 - val_loss: 68.2721\n",
      "Epoch 81/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 70.2300 - val_loss: 85.1912\n",
      "Epoch 82/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 72.9017 - val_loss: 92.4319\n",
      "Epoch 83/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 67.8034 - val_loss: 70.2066\n",
      "Epoch 84/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 69.5453 - val_loss: 85.2819\n",
      "Epoch 85/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.5404 - val_loss: 66.7745\n",
      "Epoch 86/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 72.1602 - val_loss: 86.0502\n",
      "Epoch 87/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 67.9959 - val_loss: 72.7569\n",
      "Epoch 88/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 66.9991 - val_loss: 85.2339\n",
      "Epoch 89/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 70.2285 - val_loss: 76.7269\n",
      "Epoch 90/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 70.7189 - val_loss: 78.5281\n",
      "Epoch 91/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 65.3865 - val_loss: 87.5600\n",
      "Epoch 92/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 70.7044 - val_loss: 59.1678\n",
      "Epoch 93/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 66.0940 - val_loss: 86.1420\n",
      "Epoch 94/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 68.1309 - val_loss: 95.9293\n",
      "Epoch 95/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 66.3670 - val_loss: 70.7516\n",
      "Epoch 96/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 66.0242 - val_loss: 77.9558\n",
      "Epoch 97/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 71.3772 - val_loss: 69.6898\n",
      "Epoch 98/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.2921 - val_loss: 91.2766\n",
      "Epoch 99/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 67.0623 - val_loss: 86.7455\n",
      "Epoch 100/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 70.0554 - val_loss: 82.9995\n",
      "Epoch 101/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 64.4182 - val_loss: 75.9985\n",
      "Epoch 102/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 68.8148 - val_loss: 78.4005\n",
      "Epoch 103/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 67.5404 - val_loss: 56.4634\n",
      "Epoch 104/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.3544 - val_loss: 85.4591\n",
      "Epoch 105/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 70.7299 - val_loss: 80.0693\n",
      "Epoch 106/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.0563 - val_loss: 70.7916\n",
      "Epoch 107/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 67.3198 - val_loss: 84.4549\n",
      "Epoch 108/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 66.7047 - val_loss: 75.8938\n",
      "Epoch 109/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.7800 - val_loss: 84.0139\n",
      "Epoch 110/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 67.9519 - val_loss: 99.0198\n",
      "Epoch 111/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.6611 - val_loss: 80.3460\n",
      "Epoch 112/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 69.9600 - val_loss: 84.0957\n",
      "Epoch 113/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.8239 - val_loss: 78.1213\n",
      "Epoch 114/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 65.8906 - val_loss: 70.6906\n",
      "Epoch 115/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 68.4492 - val_loss: 68.6721\n",
      "Epoch 116/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 67.7940 - val_loss: 70.8809\n",
      "Epoch 117/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 66.6384 - val_loss: 90.3675\n",
      "Epoch 118/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.3295 - val_loss: 60.2113\n",
      "Epoch 119/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 65.7363 - val_loss: 67.9427\n",
      "Epoch 120/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 64.3339 - val_loss: 77.5095\n",
      "Epoch 121/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 65.7891 - val_loss: 89.0338\n",
      "Epoch 122/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 64.5191 - val_loss: 94.9882\n",
      "Epoch 123/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.9931 - val_loss: 68.7568\n",
      "Epoch 124/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 71.4354 - val_loss: 54.8184\n",
      "Epoch 125/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 66.2838 - val_loss: 59.3706\n",
      "Epoch 126/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 66.0316 - val_loss: 90.6065\n",
      "Epoch 127/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.4330 - val_loss: 57.6351\n",
      "Epoch 128/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 67.0937 - val_loss: 73.6962\n",
      "Epoch 129/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 65.8869 - val_loss: 74.5258\n",
      "Epoch 130/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 66.4370 - val_loss: 68.1343\n",
      "Epoch 131/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 64.4957 - val_loss: 80.4792\n",
      "Epoch 132/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 64.0870 - val_loss: 59.3112\n",
      "Epoch 133/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 64.0890 - val_loss: 73.8783\n",
      "Epoch 134/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 68.4540 - val_loss: 69.8927\n",
      "Epoch 135/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.4640 - val_loss: 75.4393\n",
      "Epoch 136/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 70.1643 - val_loss: 57.7670\n",
      "Epoch 137/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 59.7148 - val_loss: 82.2888\n",
      "Epoch 138/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 64.9769 - val_loss: 78.9351\n",
      "Epoch 139/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 66.8179 - val_loss: 66.7319\n",
      "Epoch 140/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 65.9084 - val_loss: 81.5139\n",
      "Epoch 141/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 64.3638 - val_loss: 80.4610\n",
      "Epoch 142/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 64.1745 - val_loss: 71.9832\n",
      "Epoch 143/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 64.6054 - val_loss: 76.2149\n",
      "Epoch 144/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 67.1413 - val_loss: 54.1890\n",
      "Epoch 145/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 60.7389 - val_loss: 63.2007\n",
      "Epoch 146/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.5311 - val_loss: 76.1799\n",
      "Epoch 147/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 61.7712 - val_loss: 70.5113\n",
      "Epoch 148/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 66.3620 - val_loss: 66.2616\n",
      "Epoch 149/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 62.1569 - val_loss: 75.7525\n",
      "Epoch 150/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.8132 - val_loss: 67.1066\n",
      "Epoch 151/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 66.5770 - val_loss: 59.4946\n",
      "Epoch 152/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.1965 - val_loss: 62.8600\n",
      "Epoch 153/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.0248 - val_loss: 77.2285\n",
      "Epoch 154/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 64.3520 - val_loss: 71.8768\n",
      "Epoch 155/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 68.5234 - val_loss: 73.0943\n",
      "Epoch 156/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.6529 - val_loss: 96.9056\n",
      "Epoch 157/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 62.8313 - val_loss: 53.2288\n",
      "Epoch 158/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.1366 - val_loss: 74.8585\n",
      "Epoch 159/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 66.3957 - val_loss: 74.2360\n",
      "Epoch 160/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 62.6445 - val_loss: 75.4011\n",
      "Epoch 161/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.1001 - val_loss: 55.5857\n",
      "Epoch 162/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 68.2158 - val_loss: 78.2768\n",
      "Epoch 163/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 62.2721 - val_loss: 83.0982\n",
      "Epoch 164/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.7561 - val_loss: 71.9817\n",
      "Epoch 165/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 62.8245 - val_loss: 75.8822\n",
      "Epoch 166/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.7566 - val_loss: 78.5561\n",
      "Epoch 167/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.5325 - val_loss: 69.4634\n",
      "Epoch 168/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.4848 - val_loss: 69.7059\n",
      "Epoch 169/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.5113 - val_loss: 99.0511\n",
      "Epoch 170/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 62.7183 - val_loss: 62.3616\n",
      "Epoch 171/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.0991 - val_loss: 71.6113\n",
      "Epoch 172/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.4047 - val_loss: 81.9292\n",
      "Epoch 173/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.0921 - val_loss: 73.8381\n",
      "Epoch 174/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.8592 - val_loss: 87.8853\n",
      "Epoch 175/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.2769 - val_loss: 61.9301\n",
      "Epoch 176/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.5488 - val_loss: 73.4509\n",
      "Epoch 177/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.6497 - val_loss: 63.4733\n",
      "Epoch 178/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.2985 - val_loss: 67.6799\n",
      "Epoch 179/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 64.8890 - val_loss: 61.8551\n",
      "Epoch 180/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.2063 - val_loss: 82.6564\n",
      "Epoch 181/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 60.4212 - val_loss: 52.6577\n",
      "Epoch 182/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 68.2487 - val_loss: 76.7710\n",
      "Epoch 183/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 59.8649 - val_loss: 75.4167\n",
      "Epoch 184/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.6944 - val_loss: 62.6680\n",
      "Epoch 185/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.6181 - val_loss: 60.0842\n",
      "Epoch 186/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.4495 - val_loss: 97.7772\n",
      "Epoch 187/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 61.0985 - val_loss: 88.7680\n",
      "Epoch 188/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 64.0814 - val_loss: 71.0795\n",
      "Epoch 189/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.4015 - val_loss: 64.1447\n",
      "Epoch 190/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.6244 - val_loss: 76.6501\n",
      "Epoch 191/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.1933 - val_loss: 63.5284\n",
      "Epoch 192/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 62.7747 - val_loss: 83.0540\n",
      "Epoch 193/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.6224 - val_loss: 65.5255\n",
      "Epoch 194/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.9219 - val_loss: 76.6982\n",
      "Epoch 195/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.7469 - val_loss: 70.6993\n",
      "Epoch 196/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.6217 - val_loss: 79.5139\n",
      "Epoch 197/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.4104 - val_loss: 74.6388\n",
      "Epoch 198/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.5534 - val_loss: 76.5418\n",
      "Epoch 199/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.5682 - val_loss: 71.6928\n",
      "Epoch 200/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.8648 - val_loss: 64.3991\n",
      "Epoch 201/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.1772 - val_loss: 78.9889\n",
      "Epoch 202/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.3285 - val_loss: 60.8240\n",
      "Epoch 203/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.4499 - val_loss: 71.0982\n",
      "Epoch 204/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.2936 - val_loss: 70.6002\n",
      "Epoch 205/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.2265 - val_loss: 70.6868\n",
      "Epoch 206/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 62.1627 - val_loss: 73.5879\n",
      "Epoch 207/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.7036 - val_loss: 87.3520\n",
      "Epoch 208/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.9018 - val_loss: 61.1255\n",
      "Epoch 209/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.7749 - val_loss: 83.6958\n",
      "Epoch 210/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.7498 - val_loss: 76.0496\n",
      "Epoch 211/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.9956 - val_loss: 71.6684\n",
      "Epoch 212/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.5432 - val_loss: 84.4956\n",
      "Epoch 213/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.3515 - val_loss: 79.8445\n",
      "Epoch 214/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.1805 - val_loss: 55.9330\n",
      "Epoch 215/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 60.5969 - val_loss: 58.5841\n",
      "Epoch 216/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 64.0241 - val_loss: 75.7261\n",
      "Epoch 217/500\n",
      "11/11 [==============================] - 41s 4s/step - loss: 64.2884 - val_loss: 71.5298\n",
      "Epoch 218/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 58.4024 - val_loss: 68.3141\n",
      "Epoch 219/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 64.6091 - val_loss: 77.8063\n",
      "Epoch 220/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.3119 - val_loss: 69.1893\n",
      "Epoch 221/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.7595 - val_loss: 87.5778\n",
      "Epoch 222/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.5414 - val_loss: 71.7595\n",
      "Epoch 223/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.2748 - val_loss: 56.0106\n",
      "Epoch 224/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.1250 - val_loss: 81.2564\n",
      "Epoch 225/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.9963 - val_loss: 79.0783\n",
      "Epoch 226/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.6381 - val_loss: 45.2580\n",
      "Epoch 227/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.8225 - val_loss: 84.4495\n",
      "Epoch 228/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 61.5997 - val_loss: 57.8596\n",
      "Epoch 229/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.4831 - val_loss: 76.2587\n",
      "Epoch 230/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.9466 - val_loss: 56.2206\n",
      "Epoch 231/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 62.8609 - val_loss: 53.3460\n",
      "Epoch 232/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.2975 - val_loss: 78.6136\n",
      "Epoch 233/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.1152 - val_loss: 65.2823\n",
      "Epoch 234/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.2293 - val_loss: 67.7934\n",
      "Epoch 235/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 62.8938 - val_loss: 74.6087\n",
      "Epoch 236/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.6821 - val_loss: 62.0314\n",
      "Epoch 237/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.5381 - val_loss: 68.5432\n",
      "Epoch 238/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.2430 - val_loss: 71.7088\n",
      "Epoch 239/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.6976 - val_loss: 86.2374\n",
      "Epoch 240/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 59.5865 - val_loss: 67.1541\n",
      "Epoch 241/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 66.6137 - val_loss: 72.6212\n",
      "Epoch 242/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.0904 - val_loss: 57.9966\n",
      "Epoch 243/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.8265 - val_loss: 87.0747\n",
      "Epoch 244/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 62.1816 - val_loss: 84.1173\n",
      "Epoch 245/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.7688 - val_loss: 60.4428\n",
      "Epoch 246/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.2909 - val_loss: 60.3661\n",
      "Epoch 247/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.2261 - val_loss: 71.7024\n",
      "Epoch 248/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.4921 - val_loss: 65.6396\n",
      "Epoch 249/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.9551 - val_loss: 61.8351\n",
      "Epoch 250/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.5094 - val_loss: 86.3712\n",
      "Epoch 251/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.3549 - val_loss: 82.6306\n",
      "Epoch 252/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.0649 - val_loss: 81.8729\n",
      "Epoch 253/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 66.3253 - val_loss: 73.8552\n",
      "Epoch 254/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.8096 - val_loss: 81.9500\n",
      "Epoch 255/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.4017 - val_loss: 80.0187\n",
      "Epoch 256/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.4624 - val_loss: 73.5953\n",
      "Epoch 257/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.2213 - val_loss: 105.2482\n",
      "Epoch 258/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.6865 - val_loss: 58.0840\n",
      "Epoch 259/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.1636 - val_loss: 74.6847\n",
      "Epoch 260/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.8265 - val_loss: 56.2739\n",
      "Epoch 261/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.1695 - val_loss: 76.4133\n",
      "Epoch 262/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 62.4404 - val_loss: 61.9735\n",
      "Epoch 263/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.2157 - val_loss: 81.0126\n",
      "Epoch 264/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.0219 - val_loss: 74.5832\n",
      "Epoch 265/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.7123 - val_loss: 83.8166\n",
      "Epoch 266/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.6878 - val_loss: 64.9963\n",
      "Epoch 267/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.0622 - val_loss: 43.9265\n",
      "Epoch 268/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.7341 - val_loss: 57.1986\n",
      "Epoch 269/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.1574 - val_loss: 92.9835\n",
      "Epoch 270/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.3035 - val_loss: 66.5821\n",
      "Epoch 271/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 62.5156 - val_loss: 76.4823\n",
      "Epoch 272/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.7534 - val_loss: 69.6683\n",
      "Epoch 273/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 64.7512 - val_loss: 59.4813\n",
      "Epoch 274/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 57.8833 - val_loss: 74.0249\n",
      "Epoch 275/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.6923 - val_loss: 101.0013\n",
      "Epoch 276/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.4532 - val_loss: 70.4292\n",
      "Epoch 277/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 64.5153 - val_loss: 57.1420\n",
      "Epoch 278/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 57.7665 - val_loss: 67.6832\n",
      "Epoch 279/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 64.3311 - val_loss: 69.6440\n",
      "Epoch 280/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.1623 - val_loss: 91.5805\n",
      "Epoch 281/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.7949 - val_loss: 58.8170\n",
      "Epoch 282/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.4416 - val_loss: 84.1582\n",
      "Epoch 283/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.9903 - val_loss: 74.1911\n",
      "Epoch 284/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 60.2884 - val_loss: 97.6966\n",
      "Epoch 285/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 64.6179 - val_loss: 56.1778\n",
      "Epoch 286/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.0710 - val_loss: 70.2180\n",
      "Epoch 287/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 63.3082 - val_loss: 71.4522\n",
      "Epoch 288/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.3579 - val_loss: 86.8042\n",
      "Epoch 289/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.5165 - val_loss: 78.6284\n",
      "Epoch 290/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.8698 - val_loss: 77.2911\n",
      "Epoch 291/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.6132 - val_loss: 54.5384\n",
      "Epoch 292/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.7380 - val_loss: 64.7725\n",
      "Epoch 293/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 63.6922 - val_loss: 71.0392\n",
      "Epoch 294/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.1450 - val_loss: 84.5562\n",
      "Epoch 295/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 64.5524 - val_loss: 66.2880\n",
      "Epoch 296/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 57.0748 - val_loss: 62.5540\n",
      "Epoch 297/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 63.3403 - val_loss: 73.8472\n",
      "Epoch 298/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 60.5062 - val_loss: 59.4264\n",
      "Epoch 299/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.7224 - val_loss: 76.6772\n",
      "Epoch 300/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.9770 - val_loss: 68.0287\n",
      "Epoch 301/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 66.1131 - val_loss: 48.3323\n",
      "Epoch 302/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.3709 - val_loss: 62.9178\n",
      "Epoch 303/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.5386 - val_loss: 64.8545\n",
      "Epoch 304/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.6971 - val_loss: 77.5935\n",
      "Epoch 305/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 62.2032 - val_loss: 66.3731\n",
      "Epoch 306/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 60.2115 - val_loss: 62.3426\n",
      "Epoch 307/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.6689 - val_loss: 75.1199\n",
      "Epoch 308/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 58.0720 - val_loss: 84.1101\n",
      "Epoch 309/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.0661 - val_loss: 78.4355\n",
      "Epoch 310/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 60.3374 - val_loss: 64.0977\n",
      "Epoch 311/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 66.9426 - val_loss: 65.0181\n",
      "Epoch 312/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.8053 - val_loss: 62.6475\n",
      "Epoch 313/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.9859 - val_loss: 74.5877\n",
      "Epoch 314/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 57.6219 - val_loss: 92.2054\n",
      "Epoch 315/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.0477 - val_loss: 82.8706\n",
      "Epoch 316/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 62.4654 - val_loss: 74.0014\n",
      "Epoch 317/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.1247 - val_loss: 67.8432\n",
      "Epoch 318/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.2493 - val_loss: 70.3486\n",
      "Epoch 319/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.4055 - val_loss: 82.4042\n",
      "Epoch 320/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.9461 - val_loss: 67.3263\n",
      "Epoch 321/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 62.9362 - val_loss: 58.3188\n",
      "Epoch 322/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 53.5614 - val_loss: 71.6404\n",
      "Epoch 323/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.0830 - val_loss: 78.6693\n",
      "Epoch 324/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.0991 - val_loss: 55.6009\n",
      "Epoch 325/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.3500 - val_loss: 86.8091\n",
      "Epoch 326/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.2416 - val_loss: 77.5621\n",
      "Epoch 327/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.5860 - val_loss: 76.3051\n",
      "Epoch 328/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.6115 - val_loss: 73.8372\n",
      "Epoch 329/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.6468 - val_loss: 80.2656\n",
      "Epoch 330/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.2770 - val_loss: 73.5116\n",
      "Epoch 331/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.7407 - val_loss: 40.1285\n",
      "Epoch 332/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.2804 - val_loss: 72.5895\n",
      "Epoch 333/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.1352 - val_loss: 75.4884\n",
      "Epoch 334/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.5544 - val_loss: 62.4706\n",
      "Epoch 335/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 58.8164 - val_loss: 61.6204\n",
      "Epoch 336/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.3870 - val_loss: 79.4725\n",
      "Epoch 337/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.1994 - val_loss: 57.8026\n",
      "Epoch 338/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.2357 - val_loss: 83.1740\n",
      "Epoch 339/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.2999 - val_loss: 78.8176\n",
      "Epoch 340/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 64.2456 - val_loss: 71.2207\n",
      "Epoch 341/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.6663 - val_loss: 69.6072\n",
      "Epoch 342/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.7685 - val_loss: 62.8875\n",
      "Epoch 343/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.7988 - val_loss: 56.6427\n",
      "Epoch 344/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.3476 - val_loss: 67.2032\n",
      "Epoch 345/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.2786 - val_loss: 62.3460\n",
      "Epoch 346/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 59.4752 - val_loss: 68.2204\n",
      "Epoch 347/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.2689 - val_loss: 89.0020\n",
      "Epoch 348/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.8153 - val_loss: 69.5293\n",
      "Epoch 349/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 58.8607 - val_loss: 72.5447\n",
      "Epoch 350/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 63.7368 - val_loss: 59.1253\n",
      "Epoch 351/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.6220 - val_loss: 77.2002\n",
      "Epoch 352/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 62.7456 - val_loss: 71.3547\n",
      "Epoch 353/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.3314 - val_loss: 72.1872\n",
      "Epoch 354/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.7173 - val_loss: 80.6717\n",
      "Epoch 355/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.7418 - val_loss: 75.8454\n",
      "Epoch 356/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.1904 - val_loss: 69.1740\n",
      "Epoch 357/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 59.9919 - val_loss: 80.2351\n",
      "Epoch 358/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 63.9291 - val_loss: 66.1532\n",
      "Epoch 359/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.0217 - val_loss: 78.6905\n",
      "Epoch 360/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.5459 - val_loss: 66.8175\n",
      "Epoch 361/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.9528 - val_loss: 76.1945\n",
      "Epoch 362/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 65.8212 - val_loss: 80.6265\n",
      "Epoch 363/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 59.3398 - val_loss: 84.6056\n",
      "Epoch 364/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 58.3948 - val_loss: 51.0590\n",
      "Epoch 365/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 63.8242 - val_loss: 67.4015\n",
      "Epoch 366/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 57.8974 - val_loss: 79.9683\n",
      "Epoch 367/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 63.6365 - val_loss: 74.5267\n",
      "Epoch 368/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 58.1826 - val_loss: 60.2487\n",
      "Epoch 369/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 62.6245 - val_loss: 60.6618\n",
      "Epoch 370/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.8789 - val_loss: 53.5957\n",
      "Epoch 371/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.4833 - val_loss: 72.9658\n",
      "Epoch 372/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 58.3626 - val_loss: 64.6990\n",
      "Epoch 373/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 64.3708 - val_loss: 52.6620\n",
      "Epoch 374/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 60.2219 - val_loss: 70.8612\n",
      "Epoch 375/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 60.2727 - val_loss: 63.6637\n",
      "Epoch 376/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.3228 - val_loss: 65.9799\n",
      "Epoch 377/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 57.9772 - val_loss: 78.7327\n",
      "Epoch 378/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.3686 - val_loss: 78.9930\n",
      "Epoch 379/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.2805 - val_loss: 83.3567\n",
      "Epoch 380/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.5574 - val_loss: 72.8642\n",
      "Epoch 381/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.2256 - val_loss: 69.7545\n",
      "Epoch 382/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.1265 - val_loss: 78.7418\n",
      "Epoch 383/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.1635 - val_loss: 59.9762\n",
      "Epoch 384/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.9014 - val_loss: 75.2666\n",
      "Epoch 385/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.5202 - val_loss: 61.9443\n",
      "Epoch 386/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 58.6748 - val_loss: 63.0901\n",
      "Epoch 387/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 60.4017 - val_loss: 56.3864\n",
      "Epoch 388/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 59.0168 - val_loss: 65.1989\n",
      "Epoch 389/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 63.3716 - val_loss: 64.4937\n",
      "Epoch 390/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.9459 - val_loss: 59.8765\n",
      "Epoch 391/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.1151 - val_loss: 81.4377\n",
      "Epoch 392/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.4576 - val_loss: 83.4697\n",
      "Epoch 393/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 59.6479 - val_loss: 51.0818\n",
      "Epoch 394/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.0589 - val_loss: 82.2266\n",
      "Epoch 395/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.2824 - val_loss: 61.7691\n",
      "Epoch 396/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.2110 - val_loss: 62.7945\n",
      "Epoch 397/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.4682 - val_loss: 87.8924\n",
      "Epoch 398/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.8224 - val_loss: 69.4180\n",
      "Epoch 399/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.4699 - val_loss: 50.6445\n",
      "Epoch 400/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.5019 - val_loss: 56.9326\n",
      "Epoch 401/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.1140 - val_loss: 88.0108\n",
      "Epoch 402/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 57.8115 - val_loss: 68.1517\n",
      "Epoch 403/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.0168 - val_loss: 84.8202\n",
      "Epoch 404/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.0571 - val_loss: 82.8555\n",
      "Epoch 405/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 64.3886 - val_loss: 68.4472\n",
      "Epoch 406/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 57.5749 - val_loss: 73.9342\n",
      "Epoch 407/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.5589 - val_loss: 62.7739\n",
      "Epoch 408/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 59.3712 - val_loss: 85.2258\n",
      "Epoch 409/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 63.3603 - val_loss: 48.5217\n",
      "Epoch 410/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 58.4745 - val_loss: 71.0665\n",
      "Epoch 411/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 57.2110 - val_loss: 72.5510\n",
      "Epoch 412/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 64.4844 - val_loss: 77.4171\n",
      "Epoch 413/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 56.6102 - val_loss: 56.1641\n",
      "Epoch 414/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.6338 - val_loss: 65.6055\n",
      "Epoch 415/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 56.7131 - val_loss: 70.0233\n",
      "Epoch 416/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.7438 - val_loss: 69.5065\n",
      "Epoch 417/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.8120 - val_loss: 71.2436\n",
      "Epoch 418/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.1986 - val_loss: 68.1078\n",
      "Epoch 419/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.2992 - val_loss: 61.3772\n",
      "Epoch 420/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.0633 - val_loss: 76.0455\n",
      "Epoch 421/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.0988 - val_loss: 78.5516\n",
      "Epoch 422/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.6407 - val_loss: 92.2467\n",
      "Epoch 423/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.1592 - val_loss: 52.5540\n",
      "Epoch 424/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.9693 - val_loss: 51.9740\n",
      "Epoch 425/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.2245 - val_loss: 72.3170\n",
      "Epoch 426/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 60.1115 - val_loss: 56.1867\n",
      "Epoch 427/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.2115 - val_loss: 76.7931\n",
      "Epoch 428/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.3726 - val_loss: 50.2322\n",
      "Epoch 429/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.9381 - val_loss: 107.1991\n",
      "Epoch 430/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.5322 - val_loss: 74.1022\n",
      "Epoch 431/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 63.4012 - val_loss: 86.7859\n",
      "Epoch 432/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.4881 - val_loss: 69.8220\n",
      "Epoch 433/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.7996 - val_loss: 63.4160\n",
      "Epoch 434/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.2591 - val_loss: 71.1070\n",
      "Epoch 435/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 59.8135 - val_loss: 82.7643\n",
      "Epoch 436/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.3222 - val_loss: 70.7088\n",
      "Epoch 437/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 58.0930 - val_loss: 82.0955\n",
      "Epoch 438/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 59.1026 - val_loss: 64.0285\n",
      "Epoch 439/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 65.6036 - val_loss: 80.4345\n",
      "Epoch 440/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 57.8657 - val_loss: 70.2066\n",
      "Epoch 441/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.0314 - val_loss: 69.7238\n",
      "Epoch 442/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 59.7523 - val_loss: 71.8574\n",
      "Epoch 443/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.9543 - val_loss: 65.7979\n",
      "Epoch 444/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.2500 - val_loss: 71.0517\n",
      "Epoch 445/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 58.8182 - val_loss: 68.6247\n",
      "Epoch 446/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 59.4784 - val_loss: 62.6530\n",
      "Epoch 447/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 55.4650 - val_loss: 89.2798\n",
      "Epoch 448/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 63.1461 - val_loss: 66.1958\n",
      "Epoch 449/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 62.4025 - val_loss: 90.2297\n",
      "Epoch 450/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 58.2297 - val_loss: 62.7801\n",
      "Epoch 451/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 65.2079 - val_loss: 66.1233\n",
      "Epoch 452/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 54.5177 - val_loss: 56.3991\n",
      "Epoch 453/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 60.4228 - val_loss: 71.1606\n",
      "Epoch 454/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 57.7516 - val_loss: 69.2324\n",
      "Epoch 455/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 59.0090 - val_loss: 62.6225\n",
      "Epoch 456/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.1811 - val_loss: 65.9662\n",
      "Epoch 457/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.6570 - val_loss: 50.5356\n",
      "Epoch 458/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 61.2723 - val_loss: 78.9122\n",
      "Epoch 459/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 59.8551 - val_loss: 80.6370\n",
      "Epoch 460/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.0163 - val_loss: 68.9018\n",
      "Epoch 461/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.1851 - val_loss: 79.5304\n",
      "Epoch 462/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 61.6880 - val_loss: 70.1054\n",
      "Epoch 463/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 60.2858 - val_loss: 59.9434\n",
      "Epoch 464/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.0080 - val_loss: 79.5434\n",
      "Epoch 465/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 64.5432 - val_loss: 61.7749\n",
      "Epoch 466/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.2737 - val_loss: 87.8789\n",
      "Epoch 467/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 57.5651 - val_loss: 64.3270\n",
      "Epoch 468/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.0776 - val_loss: 39.0633\n",
      "Epoch 469/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.4298 - val_loss: 83.2197\n",
      "Epoch 470/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.9266 - val_loss: 82.8666\n",
      "Epoch 471/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.4039 - val_loss: 81.5537\n",
      "Epoch 472/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.4602 - val_loss: 60.9665\n",
      "Epoch 473/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.4155 - val_loss: 53.8134\n",
      "Epoch 474/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 63.8425 - val_loss: 82.6715\n",
      "Epoch 475/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 57.3907 - val_loss: 58.3079\n",
      "Epoch 476/500\n",
      "11/11 [==============================] - 37s 3s/step - loss: 62.3632 - val_loss: 75.2741\n",
      "Epoch 477/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.9407 - val_loss: 74.7406\n",
      "Epoch 478/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.3802 - val_loss: 51.6290\n",
      "Epoch 479/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.5050 - val_loss: 67.6918\n",
      "Epoch 480/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.0881 - val_loss: 54.6664\n",
      "Epoch 481/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.9218 - val_loss: 48.0834\n",
      "Epoch 482/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.7295 - val_loss: 69.7385\n",
      "Epoch 483/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 57.2322 - val_loss: 65.6998\n",
      "Epoch 484/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 61.7299 - val_loss: 78.6684\n",
      "Epoch 485/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 62.0464 - val_loss: 52.1255\n",
      "Epoch 486/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 57.9517 - val_loss: 44.2170\n",
      "Epoch 487/500\n",
      "11/11 [==============================] - 40s 4s/step - loss: 58.3302 - val_loss: 91.4259\n",
      "Epoch 488/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 62.6065 - val_loss: 68.5740\n",
      "Epoch 489/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 58.0513 - val_loss: 80.5663\n",
      "Epoch 490/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.9464 - val_loss: 70.7793\n",
      "Epoch 491/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.8782 - val_loss: 57.9596\n",
      "Epoch 492/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 57.5729 - val_loss: 77.0416\n",
      "Epoch 493/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 59.9634 - val_loss: 60.0007\n",
      "Epoch 494/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.4564 - val_loss: 69.4315\n",
      "Epoch 495/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.4009 - val_loss: 52.5302\n",
      "Epoch 496/500\n",
      "11/11 [==============================] - 38s 3s/step - loss: 60.6991 - val_loss: 62.5240\n",
      "Epoch 497/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 60.4999 - val_loss: 90.3716\n",
      "Epoch 498/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.0113 - val_loss: 76.3140\n",
      "Epoch 499/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 58.7117 - val_loss: 66.6331\n",
      "Epoch 500/500\n",
      "11/11 [==============================] - 39s 4s/step - loss: 61.4505 - val_loss: 70.7387\n",
      "Unfreeze all of the layers.\n",
      "Train on 382 samples, val on 95 samples, with batch size 32.\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-635a387f34de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0m_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-635a387f34de>\u001b[0m in \u001b[0;36m_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'trained_weights_final.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[32,52,52,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_12/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[32,52,52,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_12/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Mean_1/_4717]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Self-contained Python script to train YOLOv3 on your own dataset\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
    "from yolo3.utils import get_random_data\n",
    "\n",
    "\n",
    "def _main():\n",
    "    annotation_path = '_annotations.txt'  # path to Roboflow data annotations\n",
    "    log_dir = 'logs/000/'                 # where we're storing our logs\n",
    "    classes_path = '_classes.txt'         # path to Roboflow class names\n",
    "    anchors_path = 'model_data/yolo_anchors.txt'\n",
    "    class_names = get_classes(classes_path)\n",
    "    print(\"-------------------CLASS NAMES-------------------\")\n",
    "    print(class_names)\n",
    "    print(\"-------------------CLASS NAMES-------------------\")\n",
    "    num_classes = len(class_names)\n",
    "    anchors = get_anchors(anchors_path)\n",
    "\n",
    "    input_shape = (416,416) # multiple of 32, hw\n",
    "\n",
    "    is_tiny_version = len(anchors)==6 # default setting\n",
    "    if is_tiny_version:\n",
    "        model = create_tiny_model(input_shape, anchors, num_classes,\n",
    "            freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')\n",
    "    else:\n",
    "        model = create_model(input_shape, anchors, num_classes,\n",
    "            freeze_body=2, weights_path='model_data/yolo.h5') # make sure you know what you freeze\n",
    "\n",
    "    logging = TensorBoard(log_dir=log_dir)\n",
    "    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=10)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1)\n",
    "\n",
    "    val_split = 0.2 # set the size of the validation set\n",
    "    with open(annotation_path) as f:\n",
    "        lines = f.readlines()\n",
    "    np.random.seed(10101)\n",
    "    np.random.shuffle(lines)\n",
    "    np.random.seed(None)\n",
    "    num_val = int(len(lines)*val_split)\n",
    "    num_train = len(lines) - num_val\n",
    "\n",
    "    # Train with frozen layers first, to get a stable loss.\n",
    "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
    "    if True:\n",
    "        model.compile(optimizer=Adam(lr=1e-3), loss={\n",
    "            # use custom yolo_loss Lambda layer.\n",
    "            'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "        batch_size = 32\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "                steps_per_epoch=max(1, num_train//batch_size),\n",
    "                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "                validation_steps=max(1, num_val//batch_size),\n",
    "                epochs=500,\n",
    "                initial_epoch=0,\n",
    "                callbacks=[logging, checkpoint])\n",
    "        model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
    "\n",
    "    # Unfreeze and continue training, to fine-tune.\n",
    "    # Train longer if the result is not good.\n",
    "    if True:\n",
    "        for i in range(len(model.layers)):\n",
    "            model.layers[i].trainable = True\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
    "        print('Unfreeze all of the layers.')\n",
    "\n",
    "        batch_size = 32 # note that more GPU memory is required after unfreezing the body\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "            steps_per_epoch=max(1, num_train//batch_size),\n",
    "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "            validation_steps=max(1, num_val//batch_size),\n",
    "            epochs=100,\n",
    "            initial_epoch=50,\n",
    "            callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
    "        model.save_weights(log_dir + 'trained_weights_final.h5')\n",
    "\n",
    "    # Further training if needed.\n",
    "\n",
    "\n",
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "\n",
    "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/yolo.h5'):\n",
    "    '''create the training model'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
    "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
    "\n",
    "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
    "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/tiny_yolo_weights.h5'):\n",
    "    '''create the training model, for Tiny YOLOv3'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
    "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
    "\n",
    "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
    "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze the darknet body or freeze all but 2 output layers.\n",
    "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0:\n",
    "                np.random.shuffle(annotation_lines)\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niChsNHlOpYM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "final_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
